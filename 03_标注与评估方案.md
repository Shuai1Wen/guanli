# 03_æ ‡æ³¨ä¸è¯„ä¼°æ–¹æ¡ˆ

## æ–‡æ¡£ä¿¡æ¯

- **é¡¹ç›®åç§°**: æ”¿ç­–è¯­ä¹‰å› æœå›¾è°±(PSC-Graph)
- **æ¨¡å—**: æ ‡æ³¨è´¨é‡æ§åˆ¶ä¸è¯„ä¼°å±‚
- **ç‰ˆæœ¬**: v1.0
- **æ›´æ–°æ—¥æœŸ**: 2025-11-11
- **è´Ÿè´£äºº**: æ•°æ®è´¨é‡ç»„
- **å‰ç½®ä¾èµ–**: [01_æ•°æ®çˆ¬å–æ–¹æ¡ˆ.md](01_æ•°æ®çˆ¬å–æ–¹æ¡ˆ.md) å®Œæˆ

---

## ä¸€ã€æ¦‚è§ˆä¸ç›®æ ‡

### 1.1 ä¸šåŠ¡ç›®æ ‡

æœ¬æ–¹æ¡ˆæ˜¯PSC-Graphé¡¹ç›®çš„æ•°æ®è´¨é‡ä¿è¯æ¨¡å—,æ—¨åœ¨é€šè¿‡ç³»ç»ŸåŒ–çš„æ ‡æ³¨æµç¨‹å’Œä¸¥æ ¼çš„è¯„ä¼°æ ‡å‡†,ä¸ºè¯­ä¹‰æŠ½å–ã€å›¾å­¦ä¹ å’Œå› æœæ¨æ–­æä¾›é«˜è´¨é‡çš„é‡‘æ ‡å‡†æ•°æ®é›†ã€‚

**æ ¸å¿ƒä»»åŠ¡**:
- âœ… **é‡‘æ ‡å‡†æ„å»º**: å»ºç«‹500-1000æ¡é«˜è´¨é‡æ”¿ç­–è¦ç´ æ ‡æ³¨æ•°æ®
- âœ… **ä¸€è‡´æ€§ä¿è¯**: é€šè¿‡åŒäººæ ‡æ³¨+ä»²è£ç¡®ä¿Cohen's Îºâ‰¥0.80
- âœ… **è´¨é‡è¯„ä¼°**: å»ºç«‹å¤šå±‚æ¬¡è¯„æµ‹ä½“ç³»(å®ä½“/å…³ç³»/ç«¯åˆ°ç«¯)
- âœ… **äººå·¥éªŒæ”¶**: æä¾›ç³»ç»ŸåŒ–çš„äººå·¥æŠ½æŸ¥æ¸…å•ä¸éªŒæ”¶æµç¨‹
- âœ… **æŒç»­æ”¹è¿›**: é€šè¿‡è¯¯å·®åˆ†ææŒ‡å¯¼æ¨¡å‹ä¼˜åŒ–

### 1.2 è´¨é‡é—¨æ§›ä½“ç³»

```yaml
quality_thresholds:
  annotation_quality:
    entity_relation_f1: "â‰¥0.85"
    cohen_kappa: "â‰¥0.80"
    field_completeness: "â‰¥99%"

  extraction_quality:
    entity_f1: "â‰¥0.85"
    relation_f1: "â‰¥0.80"
    evidence_hit_rate: "â‰¥0.90"

  rag_quality:
    context_relevance: "â‰¥0.85"
    answer_faithfulness: "â‰¥0.90"
    answer_relevance: "â‰¥0.88"

  calibration:
    ece: "â‰¤0.05"
    conformal_coverage: "â‰¥0.90"
```

### 1.3 æ ‡æ³¨å·¥ä½œæµç¨‹

```mermaid
graph LR
    A[æ”¿ç­–æ–‡æœ¬æŠ½æ ·] --> B[åŒäººç‹¬ç«‹æ ‡æ³¨]
    B --> C[ä¸€è‡´æ€§è®¡ç®—Cohen's Îº]
    C --> D{Îºâ‰¥0.80?}
    D -->|æ˜¯| E[ä»²è£åˆ†æ­§é¡¹]
    D -->|å¦| F[æ ‡æ³¨æŒ‡å—ä¼˜åŒ–]
    F --> B
    E --> G[é‡‘æ ‡å‡†æ•°æ®é›†]
    G --> H[è´¨é‡éªŒè¯]
    H --> I[å‘å¸ƒä½¿ç”¨]
```

**é¢„è®¡æ—¶é—´**: ç¬¬1-2å‘¨é…åˆæ•°æ®çˆ¬å–åŒæ­¥è¿›è¡Œ

---

## äºŒã€é‡‘æ ‡å‡†æ ‡æ³¨è§„èŒƒ

### 2.1 æ ‡æ³¨å¯¹è±¡ä¸ç²’åº¦

#### ğŸ“‹ æ ‡æ³¨èŒƒå›´

```yaml
annotation_scope:
  document_types:
    - å›½åŠ¡é™¢æ”¿ç­–æ–‡ä»¶
    - éƒ¨å§”è§„ç« åˆ¶åº¦
    - çœçº§ç§‘æŠ€æ”¿ç­–
    - æ”¿ç­–è§£è¯»æ–‡ä»¶

  granularity:
    unit: "æ¡æ¬¾/æ®µè½(Clause)"
    definition: "ä»¥æ–‡æ¡£ä¸­çš„æ¡ã€æ¬¾ã€é¡¹ä¸ºåŸºæœ¬å•ä½"
    min_length: "â‰¥50å­—"
    max_length: "â‰¤500å­—"

  target_size:
    total: "500-1000æ¡"
    train: "80% (400-800æ¡)"
    dev: "10% (50-100æ¡)"
    test: "10% (50-100æ¡)"
```

#### ğŸ“Š æŠ½æ ·ç­–ç•¥

```python
# scripts/sample_for_annotation.py
import json, glob, random
from collections import defaultdict
from pathlib import Path

def stratified_sampling(corpus_dir, n=1000, strata_key="issuer"):
    """åˆ†å±‚æŠ½æ ·ç¡®ä¿è¦†ç›–å¤šæ ·æ€§"""
    docs = []
    for p in Path(corpus_dir).rglob("*.json"):
        doc = json.load(open(p, "r", encoding="utf-8"))
        docs.append(doc)

    # æŒ‰å‘æ–‡å•ä½åˆ†å±‚
    strata = defaultdict(list)
    for doc in docs:
        key = doc.get(strata_key, "unknown")
        strata[key].append(doc)

    # æ¯å±‚æŒ‰æ¯”ä¾‹æŠ½æ ·
    samples = []
    for key, group in strata.items():
        k = max(1, int(n * len(group) / len(docs)))
        samples.extend(random.sample(group, min(k, len(group))))

    # è¡¥è¶³åˆ°n
    if len(samples) < n:
        remaining = [d for d in docs if d not in samples]
        samples.extend(random.sample(remaining, n - len(samples)))

    return samples[:n]

def temporal_sampling(corpus_dir, n=1000):
    """æ—¶é—´åˆ†å±‚æŠ½æ ·"""
    docs = []
    for p in Path(corpus_dir).rglob("*.json"):
        doc = json.load(open(p, "r", encoding="utf-8"))
        if "pub_date" in doc:
            year = int(doc["pub_date"][:4])
            doc["year"] = year
            docs.append(doc)

    # æŒ‰å¹´ä»½åˆ†ç»„
    years = defaultdict(list)
    for doc in docs:
        years[doc["year"]].append(doc)

    # æ¯å¹´æŒ‰æ¯”ä¾‹æŠ½æ ·
    samples = []
    for year, group in sorted(years.items()):
        k = max(1, int(n * len(group) / len(docs)))
        samples.extend(random.sample(group, min(k, len(group))))

    return samples[:n]

if __name__ == "__main__":
    # æ‰§è¡Œåˆ†å±‚æŠ½æ ·
    samples = stratified_sampling("corpus", n=1000)

    # ä¿å­˜åˆ°å¾…æ ‡æ³¨ç›®å½•
    for i, doc in enumerate(samples):
        output_path = f"annotations/to_annotate/doc_{i:04d}.json"
        json.dump(doc, open(output_path, "w", encoding="utf-8"),
                  ensure_ascii=False, indent=2)

    print(f"[Sampling] Generated {len(samples)} samples")
```

### 2.2 æ ‡æ³¨è¦ç´ å®šä¹‰

#### ğŸ”‘ äº”å…ƒç»„æ ¸å¿ƒå­—æ®µ

**å®Œæ•´å­—æ®µè§„èŒƒ** (å¤ç”¨JSON Schema):

| å­—æ®µ | ç±»å‹ | å¿…é¡» | æšä¸¾å€¼/çº¦æŸ | è¯´æ˜ |
|-----|------|------|------------|------|
| `goal` | string | âœ… | - | æ”¿ç­–ç›®æ ‡/ä»»åŠ¡ |
| `instrument` | array[string] | âœ… | [funding, tax, land, talent, standard, platform, ip, finance, procurement, pilot, data_compute, other] | æ”¿ç­–å·¥å…·(å¯å¤šé€‰) |
| `target_actor` | string | âœ… | - | å¯¹è±¡/ä¸»ä½“(ä¼ä¸š/é«˜æ ¡/ç§‘ç ”é™¢æ‰€) |
| `region` | object | âŒ | {name, admin_code, uncertain} | åœ°åŸŸ/è¦†ç›–èŒƒå›´ |
| `timeframe` | object | âŒ | {effective_date, expiry_date} | æ—¶é—´/æœ‰æ•ˆæœŸ |
| `strength` | integer | âœ… | [0, 1, 2, 3] | å¼ºåº¦/çº¦æŸæ€§ |
| `support` | array[object] | âŒ | {type, value, unit} | é…å¥—æªæ–½(èµ„é‡‘/ç¨æ”¶ç­‰) |
| `evidence_spans` | array[object] | âœ… | {start, end, from_doc} | è¯æ®æ®µè½ |
| `confidence` | float | âœ… | [0.0, 1.0] | ä¸»è§‚ç½®ä¿¡åº¦ |

#### ğŸ“ å¼ºåº¦åˆ†çº§æ ‡å‡†

```yaml
strength_levels:
  level_3_strong:
    definition: "å¼ºçº¦æŸ"
    indicators:
      - æ˜ç¡®èµ„é‡‘æ‹¨ä»˜é‡‘é¢æˆ–æ¯”ä¾‹
      - å…·æœ‰è€ƒæ ¸/é—®è´£æœºåˆ¶
      - ç¡¬æ€§å‡†å…¥æ¡ä»¶æˆ–æŒ‡æ ‡
    keywords: ["å¿…é¡»", "åº”å½“", "ä¸¥æ ¼", "è€ƒæ ¸", "é—®è´£"]
    examples:
      - "å¯¹ç¬¦åˆæ¡ä»¶çš„ä¼ä¸šç»™äºˆä¸ä½äº500ä¸‡å…ƒç ”å‘è¡¥è´´"
      - "æœªè¾¾æ ‡çš„åœ°åŒºå°†å–æ¶ˆè¯•ç‚¹èµ„æ ¼"

  level_2_moderate:
    definition: "ä¸€èˆ¬æ€§çº¦æŸ"
    indicators:
      - æ˜ç¡®è´£ä»»å•ä½ä¸æ—¶é—´è¡¨
      - æœ‰æ‰§è¡Œè·¯å¾„ä½†æ— ç¡¬æ€§è€ƒæ ¸
      - åˆ¶åº¦åŒ–å®‰æ’
    keywords: ["æ˜ç¡®", "è§„å®š", "è¦æ±‚", "è½å®"]
    examples:
      - "ç”±ç§‘æŠ€å…ç‰µå¤´,äº2024å¹´åº•å‰å®Œæˆå¹³å°å»ºè®¾"
      - "å„åœ°åº”å»ºç«‹é¡¹ç›®åº“å¹¶å®šæœŸæ›´æ–°"

  level_1_advisory:
    definition: "å€¡è®®æ€§"
    indicators:
      - é¼“åŠ±æ€§/å¼•å¯¼æ€§è¡¨è¿°
      - æ— æ˜ç¡®æ‰§è¡Œè·¯å¾„
      - æ— çº¦æŸæœºåˆ¶
    keywords: ["é¼“åŠ±", "æ”¯æŒ", "å¼•å¯¼", "å€¡å¯¼"]
    examples:
      - "é¼“åŠ±ä¼ä¸šåŠ å¤§ç ”å‘æŠ•å…¥"
      - "æ”¯æŒé«˜æ ¡å¼€å±•äº§å­¦ç ”åˆä½œ"

  level_0_background:
    definition: "æ— çº¦æŸ"
    indicators:
      - èƒŒæ™¯æ€§/è§£é‡Šæ€§å†…å®¹
      - æ— å¯æ‰§è¡Œå·¥å…·
      - æ³›æ³›è€Œè°ˆ
    keywords: ["å½¢åŠ¿", "æ„ä¹‰", "é‡è¦æ€§"]
    examples:
      - "åˆ›æ–°æ˜¯å¼•é¢†å‘å±•çš„ç¬¬ä¸€åŠ¨åŠ›"
      - "åŠ å¼ºåˆ›æ–°ä½“ç³»å»ºè®¾" (æ— å…·ä½“å·¥å…·)
```

### 2.3 æ ‡æ³¨æµç¨‹ä¸è§„åˆ™

#### ğŸ”„ åŒäººæ ‡æ³¨æµç¨‹

```yaml
annotation_workflow:
  step_1_assignment:
    action: "å°†å¾…æ ‡æ³¨æ–‡æ¡£éšæœºåˆ†é…ç»™æ ‡æ³¨äººAå’ŒB"
    constraint: "ç›¸åŒæ–‡æ¡£ç”±ä¸¤äººç‹¬ç«‹æ ‡æ³¨"

  step_2_independent_annotation:
    action: "æ ‡æ³¨äººç‹¬ç«‹å®Œæˆæ ‡æ³¨"
    time_limit: "â‰¤30åˆ†é’Ÿ/æ–‡æ¡£"
    tools: "Label Studio / è‡ªç ”æ ‡æ³¨å·¥å…·"

  step_3_consistency_check:
    action: "ç³»ç»Ÿè‡ªåŠ¨è®¡ç®—ä¸€è‡´æ€§(Cohen's Îº)"
    threshold: "Îºâ‰¥0.80"

  step_4_adjudication:
    trigger: "Îº<0.80 æˆ– å­˜åœ¨åˆ†æ­§é¡¹"
    action: "ä»²è£äººè£å®šæœ€ç»ˆæ ‡æ³¨"
    requirement: "ä»²è£äººéœ€è®°å½•adjudication_note"

  step_5_finalization:
    action: "é€šè¿‡éªŒè¯ååŠ å…¥é‡‘æ ‡å‡†æ•°æ®é›†"
    location: "annotations/adjudicated/"
```

#### ğŸ“‹ æ ‡æ³¨è§„åˆ™ç»†åˆ™

**è§„åˆ™1: å…ˆæ‰¾å·¥å…·,å†æ‰¾ç›®æ ‡**
```yaml
principle: "ä¼˜å…ˆè¯†åˆ«å¯æ‰§è¡Œçš„æ”¿ç­–å·¥å…·,å†ç»‘å®šå…¶æœåŠ¡çš„ç›®æ ‡"

example:
  text: "è®¾ç«‹äººå·¥æ™ºèƒ½ä¸“é¡¹åŸºé‡‘,æ”¯æŒä¼ä¸šå¼€å±•æ ¸å¿ƒæŠ€æœ¯æ”»å…³"
  annotation:
    step_1: "è¯†åˆ«å·¥å…· = funding (ä¸“é¡¹åŸºé‡‘)"
    step_2: "è¯†åˆ«ç›®æ ‡ = æ ¸å¿ƒæŠ€æœ¯æ”»å…³"
    step_3: "è¯†åˆ«å¯¹è±¡ = ä¼ä¸š"
```

**è§„åˆ™2: è¯æ®æº¯æºå¼ºåˆ¶è¦æ±‚**
```yaml
requirement: "æ‰€æœ‰æŠ½å–å†…å®¹å¿…é¡»å¯å›æº¯åˆ°åŸæ–‡å…·ä½“ä½ç½®"

evidence_spans:
  - å¿…é¡»æä¾›startå’Œendåç§»é‡
  - è¯æ®æ–‡æœ¬åº”å®Œæ•´åŒ…å«å…³é”®ä¿¡æ¯
  - è·¨å¥/è·¨æ®µæ—¶æ ‡æ³¨å¤šä¸ªspan

example:
  text: "ç¬¬äº”æ¡ å¯¹æ–°è®¤å®šçš„å›½å®¶çº§ç ”å‘ä¸­å¿ƒ,ç»™äºˆ300ä¸‡å…ƒä¸€æ¬¡æ€§å¥–åŠ±ã€‚"
  evidence_spans:
    - {start: 0, end: 38}  # å®Œæ•´æ¡æ¬¾
```

**è§„åˆ™3: æ¨¡ç³Šä¿¡æ¯å¤„ç†**
```yaml
unclear_region:
  text: "åœ¨éƒ¨åˆ†åœ°åŒºå¼€å±•è¯•ç‚¹"
  annotation:
    region: {name: "éƒ¨åˆ†åœ°åŒº", admin_code: null, uncertain: true}

missing_amount:
  text: "ç»™äºˆè´¢æ”¿è¡¥è´´"
  annotation:
    support: [{type: "funding", value: null, unit: null, note: "é‡‘é¢æœªæ˜ç¡®"}]

ambiguous_time:
  text: "è¿‘æœŸå¯åŠ¨å®æ–½"
  annotation:
    timeframe: {effective_date: null, note: "æ—¶é—´æ¨¡ç³Š"}
```

**è§„åˆ™4: å†²çªå¤„ç†æœºåˆ¶**
```yaml
conflict_resolution:
  scenario_1:
    issue: "å·¥å…·ç±»å‹åˆ¤æ–­ä¸ä¸€è‡´"
    resolution: "ä»²è£äººå‚è€ƒæœ¯è¯­è¡¨(é™„å½•A)è£å®š"

  scenario_2:
    issue: "å¼ºåº¦åˆ†çº§äº‰è®®"
    resolution: "ä»²è£äººä¾æ®æŒ‡æ ‡å…³é”®è¯(å¿…é¡»/åº”å½“/é¼“åŠ±)è£å®š"

  scenario_3:
    issue: "è¯æ®èŒƒå›´ä¸ä¸€è‡´"
    resolution: "é‡‡ç”¨è¦†ç›–æ›´å®Œæ•´çš„span"
```

### 2.4 æ ‡æ³¨å·¥å…·é…ç½®

#### ğŸ”§ Label Studioé…ç½®

```json
{
  "title": "PSC-Graphæ”¿ç­–è¦ç´ æ ‡æ³¨",
  "type": "layout",
  "children": [
    {
      "type": "View",
      "children": [
        {
          "type": "Header",
          "value": "æ”¿ç­–æ–‡æœ¬"
        },
        {
          "type": "Text",
          "name": "text",
          "value": "$text"
        },
        {
          "type": "Header",
          "value": "æ ‡æ³¨åŒº"
        },
        {
          "type": "Labels",
          "name": "instrument",
          "toName": "text",
          "choice": "multiple",
          "children": [
            {"value": "funding", "background": "#FFA500"},
            {"value": "tax", "background": "#FFD700"},
            {"value": "land", "background": "#98FB98"},
            {"value": "talent", "background": "#87CEEB"},
            {"value": "standard", "background": "#DDA0DD"},
            {"value": "platform", "background": "#F0E68C"},
            {"value": "ip", "background": "#FFB6C1"},
            {"value": "finance", "background": "#20B2AA"},
            {"value": "procurement", "background": "#FF6347"},
            {"value": "pilot", "background": "#4682B4"},
            {"value": "data_compute", "background": "#9370DB"}
          ]
        },
        {
          "type": "TextArea",
          "name": "goal",
          "toName": "text",
          "placeholder": "è¾“å…¥æ”¿ç­–ç›®æ ‡/ä»»åŠ¡",
          "required": true
        },
        {
          "type": "TextArea",
          "name": "target_actor",
          "toName": "text",
          "placeholder": "è¾“å…¥å¯¹è±¡/ä¸»ä½“",
          "required": true
        },
        {
          "type": "Choices",
          "name": "strength",
          "toName": "text",
          "choice": "single",
          "required": true,
          "children": [
            {"value": "0 - æ— çº¦æŸ"},
            {"value": "1 - å€¡è®®"},
            {"value": "2 - ä¸€èˆ¬æ€§"},
            {"value": "3 - å¼ºçº¦æŸ"}
          ]
        },
        {
          "type": "Rating",
          "name": "confidence",
          "toName": "text",
          "maxRating": 10,
          "defaultValue": 8
        }
      ]
    }
  ]
}
```

---

## ä¸‰ã€ä¸€è‡´æ€§è¯„ä¼°(Cohen's Îº)

### 3.1 è®¡ç®—æ–¹æ³•

#### ğŸ“Š Cohen's Kappaå…¬å¼

```
Îº = (P_o - P_e) / (1 - P_e)

å…¶ä¸­:
- P_o: è§‚å¯Ÿåˆ°çš„ä¸€è‡´æ€§æ¯”ä¾‹
- P_e: å¶ç„¶ä¸€è‡´æ€§æ¯”ä¾‹
```

**è§£é‡Šæ ‡å‡†**:
```yaml
kappa_interpretation:
  excellent: "Îº âˆˆ [0.81, 1.00] - å‡ ä¹å®Œå…¨ä¸€è‡´"
  good: "Îº âˆˆ [0.61, 0.80] - å®è´¨æ€§ä¸€è‡´"
  moderate: "Îº âˆˆ [0.41, 0.60] - ä¸­ç­‰ä¸€è‡´"
  fair: "Îº âˆˆ [0.21, 0.40] - ä¸€èˆ¬ä¸€è‡´"
  poor: "Îº âˆˆ [0.00, 0.20] - è½»å¾®ä¸€è‡´"

project_threshold: "Îº â‰¥ 0.80"
```

### 3.2 è®¡ç®—è„šæœ¬

```python
# scripts/compute_cohens_kappa.py
import json, glob
import numpy as np
from sklearn.metrics import cohen_kappa_score

def flatten_annotations(doc):
    """å°†æ ‡æ³¨æ‰å¹³åŒ–ä¸ºå¯æ¯”è¾ƒçš„å…ƒç»„"""
    items = []
    for clause in doc.get("clauses", []):
        cid = clause["clause_id"]
        for ann in clause.get("annotations", []):
            # æ„é€ æ ‡æ³¨å…ƒç»„
            item = (
                cid,
                ann.get("goal", "").strip(),
                tuple(sorted(ann.get("instrument", []))),
                ann.get("target_actor", "").strip(),
                ann.get("strength", -1)
            )
            items.append(item)
    return items

def compute_agreement(annotator_a_dir, annotator_b_dir):
    """è®¡ç®—åŒäººæ ‡æ³¨ä¸€è‡´æ€§"""
    a_files = sorted(glob.glob(f"{annotator_a_dir}/*.json"))
    b_files = sorted(glob.glob(f"{annotator_b_dir}/*.json"))

    assert len(a_files) == len(b_files), "æ ‡æ³¨æ–‡ä»¶æ•°é‡ä¸ä¸€è‡´"

    kappa_scores = []
    disagreements = []

    for fa, fb in zip(a_files, b_files):
        # åŠ è½½æ ‡æ³¨
        doc_a = json.load(open(fa, "r", encoding="utf-8"))
        doc_b = json.load(open(fb, "r", encoding="utf-8"))

        # æ‰å¹³åŒ–
        items_a = set(flatten_annotations(doc_a))
        items_b = set(flatten_annotations(doc_b))

        # æ„é€ universe (æ‰€æœ‰å¯èƒ½çš„æ ‡æ³¨)
        universe = sorted(list(items_a | items_b))

        if len(universe) == 0:
            continue

        # æ„é€ äºŒå€¼å‘é‡
        y_a = np.array([1 if item in items_a else 0 for item in universe])
        y_b = np.array([1 if item in items_b else 0 for item in universe])

        # è®¡ç®—kappa
        kappa = cohen_kappa_score(y_a, y_b)
        kappa_scores.append(kappa)

        # è®°å½•åˆ†æ­§
        if kappa < 0.80:
            disagreements.append({
                "file": fa,
                "kappa": kappa,
                "only_in_a": list(items_a - items_b),
                "only_in_b": list(items_b - items_a)
            })

    # æ±‡æ€»
    mean_kappa = np.mean(kappa_scores)
    print(f"[Cohen's Îº] Mean = {mean_kappa:.3f}")
    print(f"[Pass Rate] {sum(k>=0.80 for k in kappa_scores)}/{len(kappa_scores)}")

    # ä¿å­˜åˆ†æ­§æŠ¥å‘Š
    if disagreements:
        json.dump(disagreements,
                  open("results/disagreements.json", "w", encoding="utf-8"),
                  ensure_ascii=False, indent=2)
        print(f"[Disagreements] {len(disagreements)} files with Îº<0.80")

    return mean_kappa, disagreements

if __name__ == "__main__":
    mean_kappa, _ = compute_agreement(
        "annotations/annotator_A",
        "annotations/annotator_B"
    )

    if mean_kappa >= 0.80:
        print("âœ… [PASS] Cohen's Îº â‰¥ 0.80")
    else:
        print("âŒ [FAIL] Cohen's Îº < 0.80, éœ€è¦ä¼˜åŒ–æ ‡æ³¨æŒ‡å—")
```

### 3.3 åˆ†æ­§åˆ†æä¸æ”¹è¿›

```python
# scripts/analyze_disagreements.py
import json
from collections import Counter

def analyze_disagreement_patterns(disagreements_path):
    """åˆ†æåˆ†æ­§æ¨¡å¼,æŒ‡å¯¼æ ‡æ³¨æŒ‡å—ä¼˜åŒ–"""
    disagreements = json.load(open(disagreements_path, "r", encoding="utf-8"))

    # ç»Ÿè®¡åˆ†æ­§ç±»å‹
    instrument_diff = 0
    strength_diff = 0
    goal_diff = 0
    actor_diff = 0

    for d in disagreements:
        for item_a in d["only_in_a"]:
            for item_b in d["only_in_b"]:
                if item_a[0] == item_b[0]:  # åŒä¸€æ¡æ¬¾
                    # åˆ¤æ–­åˆ†æ­§ç±»å‹
                    if item_a[1] != item_b[1]:
                        goal_diff += 1
                    if item_a[2] != item_b[2]:
                        instrument_diff += 1
                    if item_a[3] != item_b[3]:
                        actor_diff += 1
                    if item_a[4] != item_b[4]:
                        strength_diff += 1

    print(f"[Disagreement Types]")
    print(f"  - Instrument: {instrument_diff}")
    print(f"  - Strength: {strength_diff}")
    print(f"  - Goal: {goal_diff}")
    print(f"  - Actor: {actor_diff}")

    # ç”Ÿæˆæ”¹è¿›å»ºè®®
    if instrument_diff > strength_diff:
        print("ğŸ’¡ [Suggestion] ä¼˜åŒ–instrumentæšä¸¾å®šä¹‰,è¡¥å……è¾¹ç•Œæ¡ˆä¾‹")
    if strength_diff > instrument_diff:
        print("ğŸ’¡ [Suggestion] ç»†åŒ–strengthåˆ†çº§æ ‡å‡†,å¢åŠ å…³é”®è¯ç¤ºä¾‹")

if __name__ == "__main__":
    analyze_disagreement_patterns("results/disagreements.json")
```

---

## å››ã€è´¨é‡è¯„ä¼°ä½“ç³»

### 4.1 æŠ½å–è´¨é‡è¯„æµ‹

#### ğŸ“Š å®ä½“ä¸å…³ç³»F1

```python
# scripts/evaluate_extraction.py
import json, glob
from sklearn.metrics import precision_recall_fscore_support

def load_gold_standard(gold_dir):
    """åŠ è½½é‡‘æ ‡å‡†æ•°æ®é›†"""
    gold = {}
    for p in glob.glob(f"{gold_dir}/*.json"):
        doc = json.load(open(p, "r", encoding="utf-8"))
        doc_id = doc["doc_id"]
        gold[doc_id] = flatten_annotations(doc)
    return gold

def load_predictions(pred_dir):
    """åŠ è½½æ¨¡å‹é¢„æµ‹ç»“æœ"""
    preds = {}
    for p in glob.glob(f"{pred_dir}/*.json"):
        doc = json.load(open(p, "r", encoding="utf-8"))
        doc_id = doc.get("doc_id", p.split("/")[-1].replace(".json", ""))
        preds[doc_id] = flatten_annotations(doc)
    return preds

def compute_f1(gold, preds):
    """è®¡ç®—ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1"""
    # æ„é€ universe
    all_items = set()
    for items in gold.values():
        all_items.update(items)
    for items in preds.values():
        all_items.update(items)

    universe = sorted(list(all_items))

    # æ„é€ æ ‡ç­¾å‘é‡
    y_true, y_pred = [], []
    for item in universe:
        y_true.append(1 if any(item in g for g in gold.values()) else 0)
        y_pred.append(1 if any(item in p for p in preds.values()) else 0)

    # è®¡ç®—æŒ‡æ ‡
    p, r, f1, _ = precision_recall_fscore_support(
        y_true, y_pred, average='binary', zero_division=0
    )

    return {
        "precision": p,
        "recall": r,
        "f1": f1
    }

if __name__ == "__main__":
    gold = load_gold_standard("annotations/adjudicated")
    preds = load_predictions("extractions")

    metrics = compute_f1(gold, preds)

    print(f"[Extraction Quality]")
    print(f"  Precision: {metrics['precision']:.3f}")
    print(f"  Recall: {metrics['recall']:.3f}")
    print(f"  F1: {metrics['f1']:.3f}")

    if metrics['f1'] >= 0.85:
        print("âœ… [PASS] F1 â‰¥ 0.85")
    else:
        print("âŒ [FAIL] F1 < 0.85")
```

#### ğŸ“ˆ è¯æ®å‘½ä¸­ç‡

```python
# scripts/evaluate_evidence.py
def compute_evidence_hit_rate(gold, preds):
    """è®¡ç®—è¯æ®è¢«æ­£ç¡®æ£€ç´¢çš„æ¯”ä¾‹"""
    total_evidence = 0
    hit_evidence = 0

    for doc_id in gold:
        if doc_id not in preds:
            continue

        gold_items = gold[doc_id]
        pred_items = preds[doc_id]

        for g_item in gold_items:
            total_evidence += 1
            # æ£€æŸ¥é¢„æµ‹ä¸­æ˜¯å¦åŒ…å«ç›¸åŒçš„è¯æ®
            for p_item in pred_items:
                if g_item[:4] == p_item[:4]:  # åŒ¹é…goal/instrument/actor/strength
                    hit_evidence += 1
                    break

    hit_rate = hit_evidence / total_evidence if total_evidence > 0 else 0
    print(f"[Evidence Hit Rate] {hit_rate:.3f} ({hit_evidence}/{total_evidence})")

    return hit_rate
```

### 4.2 RAGè´¨é‡è¯„æµ‹(ARES)

#### ğŸ“‹ ARESä¸‰ç»´æŒ‡æ ‡

```yaml
ares_metrics:
  context_relevance:
    definition: "æ£€ç´¢ä¸Šä¸‹æ–‡ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§"
    threshold: "â‰¥0.85"
    measurement: "äººå·¥æ ‡æ³¨ + è‡ªåŠ¨è¯„æµ‹æ¨¡å‹"

  answer_faithfulness:
    definition: "æŠ½å–ç­”æ¡ˆå¯¹æ£€ç´¢è¯æ®çš„å¿ å®åº¦"
    threshold: "â‰¥0.90"
    measurement: "ç­”æ¡ˆæ˜¯å¦å®Œå…¨æ¥è‡ªè¯æ®,æ— å¹»è§‰"

  answer_relevance:
    definition: "æŠ½å–ç­”æ¡ˆä¸æŸ¥è¯¢é—®é¢˜çš„ç›¸å…³æ€§"
    threshold: "â‰¥0.88"
    measurement: "ç­”æ¡ˆæ˜¯å¦ç›´æ¥å›ç­”é—®é¢˜"
```

**å‚è€ƒæ–‡çŒ®**: Saad-Falcon et al., 2023 (arXiv:2311.09476)

#### ğŸ”§ è¯„æµ‹æ•°æ®é›†æ„å»º

```python
# scripts/build_ares_eval_set.py
import json

def create_ares_eval_set(gold_dir, n=200):
    """æ„å»ºARESè¯„æµ‹æ•°æ®é›†"""
    eval_set = []

    for p in glob.glob(f"{gold_dir}/*.json")[:n]:
        doc = json.load(open(p, "r", encoding="utf-8"))

        for clause in doc["clauses"]:
            for ann in clause["annotations"]:
                # æ„é€ æŸ¥è¯¢-ä¸Šä¸‹æ–‡-ç­”æ¡ˆä¸‰å…ƒç»„
                query = f"è¯·ä»ä»¥ä¸‹æ”¿ç­–ä¸­æŠ½å–å·¥å…·å’Œç›®æ ‡: {clause['text'][:100]}..."
                context = clause['text']
                answer = {
                    "goal": ann["goal"],
                    "instrument": ann["instrument"],
                    "target_actor": ann["target_actor"]
                }

                eval_set.append({
                    "query": query,
                    "context": context,
                    "answer": json.dumps(answer, ensure_ascii=False),
                    "evidence_spans": ann["evidence_spans"]
                })

    # ä¿å­˜
    json.dump(eval_set, open("data/ares_eval_set.json", "w", encoding="utf-8"),
              ensure_ascii=False, indent=2)
    print(f"[ARES] Generated {len(eval_set)} evaluation samples")

if __name__ == "__main__":
    create_ares_eval_set("annotations/adjudicated", n=200)
```

### 4.3 æ¶ˆèå®éªŒè¯„æµ‹

**å®éªŒè®¾è®¡** (å¤ç”¨02æ–¹æ¡ˆ):

```yaml
ablation_experiments:
  baseline:
    name: "Zero-shot LLM"
    config: "çº¯GPT-4,æ— DAPT/TAPT,æ— RAG"

  ablation_1:
    name: "DAPT only"
    config: "æœ‰DAPT,æ— TAPT,æ— RAG"

  ablation_2:
    name: "DAPT+TAPT"
    config: "æœ‰DAPT+TAPT,æ— RAG"

  ablation_3:
    name: "RAG only"
    config: "æ— DAPT/TAPT,æœ‰RAG"

  full_model:
    name: "Complete System"
    config: "DAPT+TAPT+RAG+æ ¡å‡†"

evaluation_metrics:
  - entity_f1
  - relation_f1
  - evidence_hit_rate
  - inference_time
```

---

## äº”ã€äººå·¥éªŒæ”¶æ¸…å•

### 5.1 æŠ½æŸ¥æ–¹æ¡ˆ

```yaml
manual_review:
  sample_size: 50
  sampling_strategy: "åˆ†å±‚æŠ½æ ·"

  strata:
    by_strength:
      - {level: 3, samples: 20}  # å¼ºçº¦æŸ
      - {level: 2, samples: 15}  # ä¸€èˆ¬æ€§
      - {level: 1, samples: 10}  # å€¡è®®
      - {level: 0, samples: 5}   # èƒŒæ™¯

    by_instrument:
      - {type: "funding", samples: 15}
      - {type: "tax", samples: 10}
      - {type: "talent", samples: 10}
      - {type: "platform", samples: 8}
      - {type: "other", samples: 7}
```

### 5.2 éªŒæ”¶æ£€æŸ¥è¡¨

```markdown
# äººå·¥éªŒæ”¶æ£€æŸ¥è¡¨

## æ–‡æ¡£ä¿¡æ¯
- æ–‡æ¡£ID: _______________
- æ ‡é¢˜: _______________
- å®¡æŸ¥äºº: _______________
- å®¡æŸ¥æ—¥æœŸ: _______________

## æ£€æŸ¥é¡¹

### 1. å­—æ®µå®Œæ•´æ€§
- [ ] goalå­—æ®µéç©ºä¸”æœ‰æ„ä¹‰
- [ ] instrumentè‡³å°‘é€‰æ‹©ä¸€é¡¹
- [ ] target_actoræ˜ç¡®
- [ ] strengthåœ¨[0,3]èŒƒå›´å†…
- [ ] evidence_spanséç©º
- [ ] confidenceåœ¨[0,1]èŒƒå›´å†…

### 2. å†…å®¹å‡†ç¡®æ€§
- [ ] goalå‡†ç¡®åæ˜ æ”¿ç­–ç›®æ ‡
- [ ] instrumentç±»å‹é€‰æ‹©æ­£ç¡®
- [ ] target_actorè¯†åˆ«æ— è¯¯
- [ ] strengthåˆ†çº§åˆç†

### 3. è¯æ®æœ‰æ•ˆæ€§
- [ ] evidence_spanså¯å›æº¯åˆ°åŸæ–‡
- [ ] è¯æ®æ–‡æœ¬åŒ…å«æŠ½å–çš„å…³é”®ä¿¡æ¯
- [ ] è¯æ®èŒƒå›´å®Œæ•´(ä¸æˆªæ–­å…³é”®ä¿¡æ¯)

### 4. é€»è¾‘ä¸€è‡´æ€§
- [ ] goalä¸instrumentç›¸åŒ¹é…
- [ ] instrumentä¸target_actorç›¸å…³
- [ ] strengthä¸æè¿°ç”¨è¯ä¸€è‡´

### 5. ç‰¹æ®Šæƒ…å†µå¤„ç†
- [ ] æ¨¡ç³Šä¿¡æ¯æ ‡è®°uncertain=true
- [ ] ç¼ºå¤±å€¼ä½¿ç”¨nullè€Œéç©ºå­—ç¬¦ä¸²
- [ ] è·¨å¥è¯æ®æ ‡æ³¨å¤šä¸ªspan

## é”™è¯¯åˆ†ç±»
- [ ] æ— é”™è¯¯
- [ ] è½»å¾®é”™è¯¯(ä¸å½±å“ä½¿ç”¨)
- [ ] ä¸­ç­‰é”™è¯¯(éœ€è¦ä¿®æ­£)
- [ ] ä¸¥é‡é”™è¯¯(å®Œå…¨é”™è¯¯)

## é”™è¯¯æè¿°
_______________________________________________
_______________________________________________

## éªŒæ”¶ç»“è®º
- [ ] é€šè¿‡
- [ ] éœ€è¦ä¿®æ­£åå¤å®¡
- [ ] ä¸é€šè¿‡

å®¡æŸ¥äººç­¾å: _______________ æ—¥æœŸ: _______________
```

### 5.3 éªŒæ”¶è„šæœ¬

```python
# scripts/manual_review.py
import json, glob, random

def generate_review_batch(extraction_dir, n=50, output="review_batch.html"):
    """ç”Ÿæˆäººå·¥éªŒæ”¶æ‰¹æ¬¡"""
    files = glob.glob(f"{extraction_dir}/*.json")
    sample_files = random.sample(files, min(n, len(files)))

    html = """
    <html>
    <head>
        <meta charset="utf-8">
        <style>
            .review-item { border: 1px solid #ccc; padding: 20px; margin: 20px; }
            .field { margin: 10px 0; }
            .label { font-weight: bold; }
            .evidence { background: #ffffcc; padding: 5px; }
            .checkbox { margin: 5px 0; }
        </style>
    </head>
    <body>
        <h1>äººå·¥éªŒæ”¶æ‰¹æ¬¡ (å…±{n}æ¡)</h1>
    """.format(n=len(sample_files))

    for i, path in enumerate(sample_files):
        doc = json.load(open(path, "r", encoding="utf-8"))

        html += f"""
        <div class="review-item">
            <h2>æ ·æœ¬ {i+1}/{len(sample_files)}</h2>
            <div class="field"><span class="label">æ–‡æ¡£ID:</span> {doc.get('doc_id')}</div>
            <div class="field"><span class="label">æ ‡é¢˜:</span> {doc.get('title')}</div>

            <h3>æ ‡æ³¨å†…å®¹</h3>
        """

        for clause in doc.get("clauses", []):
            for ann in clause.get("annotations", []):
                html += f"""
                <div class="field"><span class="label">Goal:</span> {ann.get('goal')}</div>
                <div class="field"><span class="label">Instrument:</span> {', '.join(ann.get('instrument', []))}</div>
                <div class="field"><span class="label">Target Actor:</span> {ann.get('target_actor')}</div>
                <div class="field"><span class="label">Strength:</span> {ann.get('strength')}</div>
                <div class="field"><span class="label">Evidence:</span>
                    <div class="evidence">{clause['text'][ann['evidence_spans'][0]['start']:ann['evidence_spans'][0]['end']] if ann.get('evidence_spans') else 'N/A'}</div>
                </div>
                """

        html += """
            <h3>æ£€æŸ¥é¡¹</h3>
            <div class="checkbox">â˜ å­—æ®µå®Œæ•´æ€§</div>
            <div class="checkbox">â˜ å†…å®¹å‡†ç¡®æ€§</div>
            <div class="checkbox">â˜ è¯æ®æœ‰æ•ˆæ€§</div>
            <div class="checkbox">â˜ é€»è¾‘ä¸€è‡´æ€§</div>

            <h3>éªŒæ”¶ç»“è®º</h3>
            <div class="checkbox">â˜ é€šè¿‡ â˜ éœ€ä¿®æ­£ â˜ ä¸é€šè¿‡</div>
            <div><label>å¤‡æ³¨:</label> <textarea rows="3" cols="80"></textarea></div>
        </div>
        """

    html += "</body></html>"

    with open(output, "w", encoding="utf-8") as f:
        f.write(html)

    print(f"[Manual Review] Generated {output} with {len(sample_files)} samples")

if __name__ == "__main__":
    generate_review_batch("extractions", n=50, output="results/review_batch.html")
```

---

## å…­ã€è¯¯å·®åˆ†æä¸æ”¹è¿›

### 6.1 é”™è¯¯ç±»å‹åˆ†ç±»

```yaml
error_taxonomy:
  type_1_entity_errors:
    description: "å®ä½“è¯†åˆ«é”™è¯¯"
    subtypes:
      - é—æ¼å…³é”®å®ä½“
      - é”™è¯¯è¯†åˆ«éå®ä½“
      - è¾¹ç•Œè¯†åˆ«é”™è¯¯

  type_2_relation_errors:
    description: "å…³ç³»æŠ½å–é”™è¯¯"
    subtypes:
      - goalä¸instrumentä¸åŒ¹é…
      - target_actorè¯†åˆ«é”™è¯¯
      - strengthåˆ†çº§é”™è¯¯

  type_3_evidence_errors:
    description: "è¯æ®æº¯æºé”™è¯¯"
    subtypes:
      - evidence_spansä¸å®Œæ•´
      - è¯æ®ä¸æŠ½å–å†…å®¹ä¸ç¬¦
      - è·¨å¥è¯æ®æ ‡æ³¨ç¼ºå¤±

  type_4_format_errors:
    description: "æ ¼å¼è§„èŒƒé”™è¯¯"
    subtypes:
      - JSON SchemaéªŒè¯å¤±è´¥
      - å¿…é¡»å­—æ®µç¼ºå¤±
      - æšä¸¾å€¼ä¸è§„èŒƒ
```

### 6.2 è¯¯å·®åˆ†æè„šæœ¬

```python
# scripts/error_analysis.py
import json, glob
from collections import Counter

def analyze_errors(gold_dir, pred_dir):
    """åˆ†ææ¨¡å‹é¢„æµ‹é”™è¯¯"""
    gold = load_gold_standard(gold_dir)
    preds = load_predictions(pred_dir)

    errors = {
        "false_negatives": [],  # æ¼æŠ½
        "false_positives": [],  # è¯¯æŠ½
        "strength_errors": [],  # å¼ºåº¦é”™è¯¯
        "instrument_errors": []  # å·¥å…·ç±»å‹é”™è¯¯
    }

    for doc_id in gold:
        if doc_id not in preds:
            errors["false_negatives"].extend(gold[doc_id])
            continue

        gold_items = set(gold[doc_id])
        pred_items = set(preds[doc_id])

        # æ¼æŠ½
        fn = gold_items - pred_items
        errors["false_negatives"].extend(fn)

        # è¯¯æŠ½
        fp = pred_items - gold_items
        errors["false_positives"].extend(fp)

        # å¼ºåº¦é”™è¯¯
        for g in gold_items:
            for p in pred_items:
                if g[:3] == p[:3] and g[4] != p[4]:  # åŒä¸€æ¡æ¬¾,å¼ºåº¦ä¸åŒ
                    errors["strength_errors"].append((g, p))

    # ç»Ÿè®¡
    print(f"[Error Analysis]")
    print(f"  False Negatives: {len(errors['false_negatives'])}")
    print(f"  False Positives: {len(errors['false_positives'])}")
    print(f"  Strength Errors: {len(errors['strength_errors'])}")

    # åˆ†æé«˜é¢‘é”™è¯¯æ¨¡å¼
    instrument_counter = Counter()
    for item in errors["false_negatives"]:
        instrument_counter.update(item[2])  # instrument tuple

    print(f"\n[Most Missed Instruments]")
    for inst, count in instrument_counter.most_common(5):
        print(f"  - {inst}: {count}")

    return errors

if __name__ == "__main__":
    errors = analyze_errors("annotations/adjudicated", "extractions")

    # ä¿å­˜é”™è¯¯æ¡ˆä¾‹
    json.dump(errors, open("results/error_analysis.json", "w", encoding="utf-8"),
              ensure_ascii=False, indent=2, default=list)
```

### 6.3 æ”¹è¿›æªæ–½

```yaml
improvement_strategies:
  high_fn_rate:
    issue: "æ¼æŠ½ç‡é«˜(å¬å›ç‡ä½)"
    solutions:
      - å¢åŠ few-shotç¤ºä¾‹æ•°é‡
      - ä¼˜åŒ–RAGæ£€ç´¢å‚æ•°(æé«˜å¬å›)
      - è¡¥å……TAPTè®­ç»ƒæ•°æ®

  high_fp_rate:
    issue: "è¯¯æŠ½ç‡é«˜(ç²¾ç¡®ç‡ä½)"
    solutions:
      - å¼ºåŒ–è¯æ®æº¯æºçº¦æŸ
      - æé«˜ç½®ä¿¡åº¦é˜ˆå€¼
      - å¢åŠ åå¤„ç†éªŒè¯è§„åˆ™

  strength_confusion:
    issue: "å¼ºåº¦åˆ†çº§æ··æ·†"
    solutions:
      - è¡¥å……å¼ºåº¦åˆ†çº§è®­ç»ƒæ ·æœ¬
      - ä¼˜åŒ–æç¤ºè¯ä¸­çš„å…³é”®è¯åˆ—è¡¨
      - å¼•å…¥å¤šè½®å¯¹è¯ç¡®è®¤

  instrument_misclass:
    issue: "å·¥å…·ç±»å‹è¯¯åˆ¤"
    solutions:
      - ç»†åŒ–instrumentæšä¸¾å®šä¹‰
      - è¡¥å……è¾¹ç•Œæ¡ˆä¾‹åˆ°few-shot
      - æ„å»ºæœ¯è¯­-å·¥å…·æ˜ å°„è¡¨
```

---

## ä¸ƒã€å®æ–½æ­¥éª¤

### 7.1 Week 1: æ ‡æ³¨å‡†å¤‡

**Day 1-2: æŠ½æ ·ä¸åˆ†é…**
```bash
# æ‰§è¡Œåˆ†å±‚æŠ½æ ·
python scripts/sample_for_annotation.py \
    --corpus corpus \
    --output annotations/to_annotate \
    --n 1000 \
    --strategy stratified

# éšæœºåˆ†é…ç»™æ ‡æ³¨äººAå’ŒB
python scripts/assign_annotators.py \
    --input annotations/to_annotate \
    --output_a annotations/annotator_A \
    --output_b annotations/annotator_B
```

**Day 3-5: æ ‡æ³¨åŸ¹è®­**
```yaml
training_agenda:
  session_1: "æ ‡æ³¨æŒ‡å—è®²è§£(2å°æ—¶)"
  session_2: "ç¤ºä¾‹æ ‡æ³¨ç»ƒä¹ (2å°æ—¶)"
  session_3: "è¯•æ ‡æ³¨ä¸åé¦ˆ(10æ¡)"
  session_4: "æ­£å¼æ ‡æ³¨å¯åŠ¨"
```

### 7.2 Week 2: æ ‡æ³¨æ‰§è¡Œä¸è´¨æ§

**Day 6-10: åŒäººæ ‡æ³¨**
```yaml
annotation_schedule:
  daily_quota: "100æ¡/äºº"
  quality_check: "æ¯æ—¥æœ«å°¾æŠ½æŸ¥10æ¡"
  progress_tracking: "æ¯æ—¥æŠ¥å‘Šå®Œæˆè¿›åº¦"

  tools:
    - Label Studio Webç•Œé¢
    - æ ‡æ³¨æŒ‡å—PDF
    - æœ¯è¯­è¡¨Excel
```

**Day 11-12: ä¸€è‡´æ€§è®¡ç®—ä¸ä»²è£**
```bash
# è®¡ç®—Cohen's Îº
python scripts/compute_cohens_kappa.py \
    --annotator_a annotations/annotator_A \
    --annotator_b annotations/annotator_B \
    --output results/kappa_report.json

# å¦‚æœÎº<0.80,åˆ†æåˆ†æ­§æ¨¡å¼
python scripts/analyze_disagreements.py \
    --disagreements results/disagreements.json

# ä»²è£åˆ†æ­§é¡¹
python scripts/adjudication_ui.py \
    --disagreements results/disagreements.json \
    --output annotations/adjudicated
```

### 7.3 Week 3-4: è´¨é‡éªŒè¯(é…åˆ02æ–¹æ¡ˆ)

**Day 13-14: æŠ½å–è´¨é‡è¯„æµ‹**
```bash
# åœ¨æµ‹è¯•é›†ä¸Šè¯„æµ‹æŠ½å–æ¨¡å‹
python scripts/evaluate_extraction.py \
    --gold annotations/adjudicated \
    --predictions extractions \
    --output results/extraction_metrics.json

# å¦‚æœF1<0.85,æ‰§è¡Œè¯¯å·®åˆ†æ
python scripts/error_analysis.py \
    --gold annotations/adjudicated \
    --predictions extractions \
    --output results/error_analysis.json
```

---

## å…«ã€éªŒæ”¶æ ‡å‡†

### 8.1 æ ‡æ³¨è´¨é‡

```yaml
annotation_quality:
  completeness:
    metric: "å­—æ®µå®Œæ•´æ€§"
    threshold: "â‰¥99%"
    measurement: "å¿…é¡»å­—æ®µéç©ºæ¯”ä¾‹"

  consistency:
    metric: "Cohen's Îº"
    threshold: "â‰¥0.80"
    measurement: "åŒäººæ ‡æ³¨ä¸€è‡´æ€§"

  accuracy:
    metric: "äººå·¥æŠ½æŸ¥å‡†ç¡®ç‡"
    threshold: "â‰¥95%"
    sample_size: 50
```

### 8.2 æŠ½å–è´¨é‡

```yaml
extraction_quality:
  entity_f1:
    threshold: "â‰¥0.85"
    measurement: "å®ä½“è¯†åˆ«F1"

  relation_f1:
    threshold: "â‰¥0.80"
    measurement: "äº”å…ƒç»„å®Œæ•´åŒ¹é…F1"

  evidence_hit_rate:
    threshold: "â‰¥0.90"
    measurement: "è¯æ®è¢«æ£€ç´¢åˆ°çš„æ¯”ä¾‹"
```

### 8.3 RAGè´¨é‡

```yaml
rag_quality:
  context_relevance:
    threshold: "â‰¥0.85"
    measurement: "ARESä¸Šä¸‹æ–‡ç›¸å…³æ€§"

  answer_faithfulness:
    threshold: "â‰¥0.90"
    measurement: "ARESç­”æ¡ˆå¿ å®åº¦"

  answer_relevance:
    threshold: "â‰¥0.88"
    measurement: "ARESç­”æ¡ˆç›¸å…³æ€§"
```

---

## ä¹ã€å¸¸è§é—®é¢˜ä¸æ•…éšœæ’é™¤

### Q1: Cohen's ÎºæŒç»­ä½äº0.80

**è¯Šæ–­**:
```bash
python scripts/analyze_disagreements.py --disagreements results/disagreements.json
```

**è§£å†³æ–¹æ¡ˆ**:
1. å¬å¼€æ ‡æ³¨è®¨è®ºä¼š,ç»Ÿä¸€ç†è§£
2. è¡¥å……è¾¹ç•Œæ¡ˆä¾‹åˆ°æ ‡æ³¨æŒ‡å—
3. å¢åŠ æ ‡æ³¨å‰åŸ¹è®­æ—¶é—´
4. è€ƒè™‘æ›´æ¢æ ‡æ³¨å·¥å…·(æé«˜æ“ä½œä¾¿åˆ©æ€§)

### Q2: æŠ½å–F1ä½äº0.85

**æ’æŸ¥æ­¥éª¤**:
```python
# 1. æ£€æŸ¥é”™è¯¯ç±»å‹åˆ†å¸ƒ
python scripts/error_analysis.py

# 2. å¦‚æœFNç‡é«˜(å¬å›ä½)
#    â†’ å¢åŠ few-shotç¤ºä¾‹,ä¼˜åŒ–RAGå¬å›å‚æ•°

# 3. å¦‚æœFPç‡é«˜(ç²¾ç¡®ç‡ä½)
#    â†’ å¼ºåŒ–è¯æ®çº¦æŸ,æé«˜ç½®ä¿¡åº¦é˜ˆå€¼
```

### Q3: æ ‡æ³¨æ•ˆç‡ä½ä¸‹

**ä¼˜åŒ–æªæ–½**:
```yaml
efficiency_improvements:
  pre_annotation:
    action: "ä½¿ç”¨æ¨¡å‹é¢„æ ‡æ³¨,äººå·¥ä¿®æ­£"
    speedup: "2-3å€"

  active_learning:
    action: "ä¼˜å…ˆæ ‡æ³¨ä¸ç¡®å®šæ€§é«˜çš„æ ·æœ¬"
    benefit: "å‡å°‘æ ‡æ³¨é‡20-30%"

  batch_labeling:
    action: "ç›¸ä¼¼æ–‡æ¡£æ‰¹é‡æ ‡æ³¨"
    benefit: "å¤ç”¨æ€è·¯,æé«˜é€Ÿåº¦"
```

---

## åã€é™„å½•

### é™„å½•A: æ ‡æ³¨æŒ‡å—å®Œæ•´ç‰ˆ

**è¯·å‚è€ƒé¡¹ç›®æ–¹æ¡ˆç»†èŠ‚.txtç¬¬AèŠ‚**,åŒ…å«:
- æ ‡æ³¨å¯¹è±¡ä¸ç²’åº¦
- äº”å…ƒç»„å­—æ®µå®šä¹‰
- å¼ºåº¦åˆ†çº§æ ‡å‡†
- æ ‡æ³¨æµç¨‹ä¸è§„åˆ™
- è¾¹ç•Œä¸ä¾‹å¤–å¤„ç†

### é™„å½•B: æœ¯è¯­-å·¥å…·æ˜ å°„è¡¨

```yaml
terminology_mapping:
  funding:
    keywords: ["è¡¥è´´", "èµ„åŠ©", "ä¸“é¡¹èµ„é‡‘", "å¥–åŠ±", "æ‰¶æŒèµ„é‡‘"]

  tax:
    keywords: ["ç¨æ”¶ä¼˜æƒ ", "å‡å…ç¨", "æŠµæ‰£", "é€€ç¨", "æ‰€å¾—ç¨ä¼˜æƒ "]

  talent:
    keywords: ["äººæ‰å¼•è¿›", "äººæ‰è¡¥è´´", "ä½æˆ¿è¡¥è´´", "è½æˆ·", "èŒç§°"]

  platform:
    keywords: ["å¹³å°", "å­µåŒ–å™¨", "åŠ é€Ÿå™¨", "å›­åŒº", "ä¸­å¿ƒ", "åŸºåœ°"]

  ip:
    keywords: ["çŸ¥è¯†äº§æƒ", "ä¸“åˆ©", "å•†æ ‡", "æˆæœè½¬åŒ–", "æŠ€æœ¯è½¬ç§»"]

  procurement:
    keywords: ["æ”¿åºœé‡‡è´­", "é¦–è´­", "é¦–å°å¥—", "ç¤ºèŒƒåº”ç”¨"]

  standard:
    keywords: ["æ ‡å‡†", "è§„èŒƒ", "å‡†å…¥", "è®¤è¯", "èµ„è´¨"]
```

### é™„å½•C: å‚è€ƒæ–‡çŒ®

```yaml
key_references:
  cohen_kappa:
    - "Cohen J., 1960: A Coefficient of Agreement for Nominal Scales"
    - "Landis & Koch, 1977: The Measurement of Observer Agreement"

  ares:
    - "Saad-Falcon et al., 2023: ARES Automated Evaluation Framework"
    - "URL: https://arxiv.org/abs/2311.09476"

  annotation_quality:
    - "Artstein & Poesio, 2008: Inter-Coder Agreement for Computational Linguistics"
    - "URL: https://aclanthology.org/J08-4004/"
```

---

## æ€»ç»“

æœ¬æ ‡æ³¨ä¸è¯„ä¼°æ–¹æ¡ˆæä¾›äº†**ç³»ç»ŸåŒ–ã€å¯é‡åŒ–ã€å¯å®¡è®¡**çš„è´¨é‡ä¿è¯æµç¨‹,ç¡®ä¿:

âœ… **æ ‡æ³¨å®Œæ•´æ€§**: 500-1000æ¡é‡‘æ ‡å‡†æ•°æ®,è¦†ç›–å¤šæ ·åœºæ™¯
âœ… **ä¸€è‡´æ€§ä¿è¯**: Cohen's Îºâ‰¥0.80,åŒäººæ ‡æ³¨+ä»²è£æœºåˆ¶
âœ… **è¯„æµ‹ä½“ç³»**: å®ä½“/å…³ç³»F1ã€RAGè´¨é‡ã€æ¶ˆèå®éªŒ
âœ… **äººå·¥éªŒæ”¶**: 50æ¡æŠ½æŸ¥,95%å‡†ç¡®ç‡è¦æ±‚
âœ… **æŒç»­æ”¹è¿›**: è¯¯å·®åˆ†æé©±åŠ¨æ¨¡å‹ä¼˜åŒ–

**ä¸‹ä¸€æ­¥è¡ŒåŠ¨**:
1. âœ… Day 1-2: æ‰§è¡Œåˆ†å±‚æŠ½æ ·,ç”Ÿæˆå¾…æ ‡æ³¨æ•°æ®
2. âœ… Day 3-5: æ ‡æ³¨åŸ¹è®­ä¸è¯•æ ‡æ³¨
3. âœ… Day 6-10: åŒäººç‹¬ç«‹æ ‡æ³¨
4. âœ… Day 11-12: ä¸€è‡´æ€§è®¡ç®—ä¸ä»²è£
5. âœ… Day 13-14: è´¨é‡è¯„æµ‹ä¸è¯¯å·®åˆ†æ

**è”ç³»æ–¹å¼**:
- æŠ€æœ¯æ”¯æŒ: quality@psc-graph.org
- é¡¹ç›®ç®¡ç†: pm@psc-graph.org

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**ç”Ÿæˆæ—¶é—´**: 2025-11-11
**ç»´æŠ¤è€…**: PSC-Graphæ•°æ®è´¨é‡ç»„
**æœ€åæ›´æ–°**: 2025-11-11
