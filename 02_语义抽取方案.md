# 02_è¯­ä¹‰æŠ½å–æ–¹æ¡ˆ

## æ–‡æ¡£ä¿¡æ¯

- **é¡¹ç›®åç§°**: æ”¿ç­–è¯­ä¹‰å› æœå›¾è°±(PSC-Graph)
- **æ¨¡å—**: è¯­ä¹‰æŠ½å–å±‚(NLPä¸ç»“æ„åŒ–)
- **ç‰ˆæœ¬**: v1.0
- **æ›´æ–°æ—¥æœŸ**: 2025-11-11
- **è´Ÿè´£äºº**: NLPå·¥ç¨‹ç»„
- **å‰ç½®ä¾èµ–**: [01_æ•°æ®çˆ¬å–æ–¹æ¡ˆ.md](01_æ•°æ®çˆ¬å–æ–¹æ¡ˆ.md) å®Œæˆ

---

## ä¸€ã€æ¦‚è§ˆä¸ç›®æ ‡

### 1.1 ä¸šåŠ¡ç›®æ ‡

æœ¬æ–¹æ¡ˆæ˜¯PSC-Graphé¡¹ç›®çš„æ ¸å¿ƒè¯­ä¹‰ç†è§£æ¨¡å—,æ—¨åœ¨å°†éç»“æ„åŒ–æ”¿ç­–æ–‡æœ¬è½¬åŒ–ä¸ºç»“æ„åŒ–è¦ç´ è¡¨ç¤º,ä½œä¸ºå›¾è°±æ„å»ºä¸å› æœè¯†åˆ«çš„è¾“å…¥ã€‚

**æ ¸å¿ƒä»»åŠ¡**:
- âœ… **è¯­ä¹‰æŠ½å–**: ä»æ”¿ç­–æ–‡æœ¬ä¸­æŠ½å–äº”å…ƒç»„(ç›®æ ‡ã€å·¥å…·ã€å¯¹è±¡ã€åœ°åŸŸã€æ—¶é—´)
- âœ… **åŸŸé€‚é…**: é€šè¿‡DAPT/TAPTæå‡æ”¿ç­–é¢†åŸŸçš„è¯­ä¹‰ç†è§£èƒ½åŠ›
- âœ… **æ£€ç´¢å¢å¼º**: é€šè¿‡RAG(BM25+FAISS)æä¾›è¯æ®æ”¯æŒä¸å¯è¿½æº¯æ€§
- âœ… **è´¨é‡ä¿è¯**: é€šè¿‡æ¸©åº¦ç¼©æ”¾+å…±å½¢é¢„æµ‹å®ç°ä¸ç¡®å®šæ€§é‡åŒ–
- âœ… **å®‰å…¨åˆè§„**: éµå¾ªOWASP LLM Top 10å®‰å…¨åŸºçº¿

### 1.2 æŠ€æœ¯è·¯çº¿

```mermaid
graph LR
    A[åŸå§‹æ”¿ç­–æ–‡æœ¬] --> B[DAPTåŸŸé€‚é…é¢„è®­ç»ƒ]
    B --> C[TAPTä»»åŠ¡é€‚é…é¢„è®­ç»ƒ]
    C --> D[RAGæ£€ç´¢å¢å¼º]
    D --> E[Few-shot LLMæŠ½å–]
    E --> F[JSON SchemaéªŒè¯]
    F --> G[æ¸©åº¦ç¼©æ”¾æ ¡å‡†]
    G --> H[å…±å½¢é¢„æµ‹]
    H --> I[ç»“æ„åŒ–è¾“å‡º]
```

**å…³é”®å‡è®¾(H1)**:
é¢å‘æ”¿ç­–åŸŸè¿›è¡Œ**åŸŸé€‚é…é¢„è®­ç»ƒ(DAPT/TAPT)+RAG**çš„æŠ½å–ç³»ç»Ÿ,èƒ½ç¨³å®šæå‡æ”¿ç­–è¦ç´ è¯†åˆ«ä¸å½’ç±»çš„ä¸€è‡´æ€§/å‡†ç¡®æ€§,ç›¸æ¯”çº¯é›¶æ ·æœ¬/å°‘æ ·æœ¬LLMæœ‰æ˜¾è‘—æ”¹è¿›ã€‚

**å‚è€ƒæ–‡çŒ®**:
- Gururangan et al., 2020 (ACL): "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks"
- Lewis et al., 2020 (NeurIPS): "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"

### 1.3 è¾“å…¥è¾“å‡ºè§„èŒƒ

| é¡¹ç›® | è¯´æ˜ |
|-----|------|
| **è¾“å…¥** | å·²çˆ¬å–çš„æ”¿ç­–æ–‡æœ¬(corpus/*.json),åŒ…å«titleã€content_textã€pub_dateç­‰å­—æ®µ |
| **è¾“å‡º** | ç»“æ„åŒ–äº”å…ƒç»„JSON,ç¬¦åˆschemas/policy_schema.jsonè§„èŒƒ |
| **è´¨é‡é—¨æ§›** | å®ä½“/å…³ç³»F1â‰¥0.85ã€Cohen's Îºâ‰¥0.80ã€ECEâ‰¤0.05 |
| **é¢„è®¡æ—¶é—´** | 2å‘¨(W3-W4,10ä¸ªå·¥ä½œæ—¥) |

---

## äºŒã€åŸŸé€‚é…é¢„è®­ç»ƒ(DAPT/TAPT)

### 2.1 DAPT(Domain-Adaptive Pre-Training)

#### ğŸ“‹ ç›®æ ‡

åœ¨å¤§è§„æ¨¡æ”¿ç­–/æ³•è§„/è§£è¯»è¯­æ–™ä¸Šç»§ç»­é¢„è®­ç»ƒåŸºåº§æ¨¡å‹,ä½¿å…¶é€‚åº”æ”¿ç­–é¢†åŸŸçš„è¯­è¨€åˆ†å¸ƒå’Œä¸“ä¸šæœ¯è¯­ã€‚

#### ğŸ“Š æ•°æ®å‡†å¤‡

**è¯­æ–™æ¥æº**:
```yaml
primary_sources:
  - å›½åŠ¡é™¢æ”¿ç­–æ–‡ä»¶åº“ (1986-è‡³ä»Š)
  - 31çœå¸‚ç§‘æŠ€å…æ”¿ç­–æ–‡æœ¬
  - æ”¿ç­–è§£è¯»ç¨¿ä»¶
  - æ³•å¾‹æ³•è§„æ•°æ®åº“

corpus_statistics:
  target_size: "500ä¸‡-1000ä¸‡å­—"
  time_range: "1986-è‡³ä»Š(ä»¥2009å¹´åä¸ºä¸»)"
  format: "çº¯æ–‡æœ¬,ä¿ç•™æ®µè½ç»“æ„"
  preprocessing:
    - å»é™¤HTMLæ ‡ç­¾
    - ç»Ÿä¸€æ ‡ç‚¹ç¬¦å·
    - ä¿ç•™æ”¿ç­–æ–‡å·ä¸æ—¥æœŸæ ¼å¼
```

**è¯­æ–™æ¸…æ´—è„šæœ¬**:
```python
# scripts/prep_dapt_corpus.py
import json, glob, re
from pathlib import Path

def clean_text(text):
    """æ¸…æ´—HTMLæ ‡ç­¾,ç»Ÿä¸€æ ‡ç‚¹"""
    text = re.sub(r'<[^>]+>', '', text)  # ç§»é™¤HTML
    text = re.sub(r'\s+', ' ', text)     # ç»Ÿä¸€ç©ºç™½ç¬¦
    text = text.replace('ã€€', ' ')       # å…¨è§’ç©ºæ ¼è½¬åŠè§’
    return text.strip()

def iter_corpus(corpus_dir):
    for p in Path(corpus_dir).rglob("*.json"):
        doc = json.load(open(p, "r", encoding="utf-8"))
        yield {
            "id": doc.get("sha256", doc.get("doc_id")),
            "text": clean_text(doc["content_text"]),
            "meta": {
                "title": doc.get("title"),
                "pub_date": doc.get("pub_date"),
                "issuer": doc.get("issuer")
            }
        }

def main():
    corpus = list(iter_corpus("corpus"))
    with open("data/dapt_corpus.jsonl", "w", encoding="utf-8") as f:
        for doc in corpus:
            f.write(json.dumps(doc, ensure_ascii=False) + "\n")
    print(f"[DAPT] Prepared {len(corpus)} documents")

if __name__ == "__main__":
    main()
```

#### ğŸ”§ è®­ç»ƒé…ç½®

**åŸºåº§æ¨¡å‹é€‰æ‹©**:
```yaml
model_options:
  option_1:
    name: "chinese-roberta-wwm-ext-large"
    reason: "å…¨è¯æ©ç ,æ”¿ç­–æ–‡æœ¬é•¿å¥ç†è§£èƒ½åŠ›å¼º"

  option_2:
    name: "chinese-macbert-large"
    reason: "çº é”™é¢„è®­ç»ƒä»»åŠ¡,é€‚åˆè§„èŒƒæ€§æ–‡æœ¬"

  recommended: "chinese-roberta-wwm-ext-large"
```

**è®­ç»ƒå‚æ•°**:
```yaml
training_config:
  task: "Masked Language Modeling (MLM)"
  mask_probability: 0.15

  hyperparameters:
    batch_size: 32
    learning_rate: 5e-5
    warmup_steps: 1000
    max_steps: 50000
    gradient_accumulation: 4

  hardware:
    gpus: "1x A100 40GB æˆ– 2x RTX 3090"
    mixed_precision: "fp16"

  checkpoints:
    save_every: 5000
    keep_best: 3
    metric: "perplexity"
```

**Makefileç›®æ ‡**:
```makefile
# æ·»åŠ åˆ°é¡¹ç›®Makefile
dapt:
	@$(ACT) python scripts/prep_dapt_corpus.py
	@$(ACT) python scripts/run_dapt.py \
		--model hfl/chinese-roberta-wwm-ext-large \
		--corpus data/dapt_corpus.jsonl \
		--output models/dapt_checkpoint \
		--max_steps 50000
```

#### âš ï¸ å…³é”®é£é™©

| é£é™© | è§„é¿ç­–ç•¥ |
|-----|---------|
| è¯­æ–™åˆ†å¸ƒåå·® | ç¡®ä¿æ—¶é—´è·¨åº¦è¦†ç›–1986-è‡³ä»Š,é¿å…ä»…ç”¨è¿‘æœŸæ–‡æœ¬ |
| è¿‡æ‹Ÿåˆå†å²æ”¿ç­– | ç›‘æ§éªŒè¯é›†å›°æƒ‘åº¦,early stopping |
| æœ¯è¯­è¡¨ç¼ºå¤± | æ„å»ºæ”¿ç­–ä¸“ä¸šæœ¯è¯­è¯è¡¨(è§é™„å½•A),ç”¨äºåå¤„ç† |

### 2.2 TAPT(Task-Adaptive Pre-Training)

#### ğŸ“‹ ç›®æ ‡

åœ¨å°‘é‡é‡‘æ ‡æ ‡æ³¨æ•°æ®ä¸Šç»§ç»­é¢„è®­ç»ƒ,ä½¿æ¨¡å‹é€‚åº”"æ”¿ç­–è¦ç´ æŠ½å–"è¿™ä¸€å…·ä½“ä»»åŠ¡ã€‚

#### ğŸ“Š æ•°æ®å‡†å¤‡

**é‡‘æ ‡æ•°æ®è¦æ±‚**:
```yaml
gold_standard:
  size: "500-1000æ¡æ”¿ç­–æ¡æ¬¾"
  annotation: "åŒäººæ ‡æ³¨+ä»²è£(è§03_æ ‡æ³¨ä¸è¯„ä¼°æ–¹æ¡ˆ.md)"
  format: "JSON Schema Draft 2020-12"

split:
  train: "80% (400-800æ¡)"
  dev: "10% (50-100æ¡)"
  test: "10% (50-100æ¡)"
```

**TAPTä»»åŠ¡è®¾è®¡**:
```python
# scripts/prep_tapt_task.py
import json

def format_for_tapt(annotation_path):
    """å°†é‡‘æ ‡æ ‡æ³¨è½¬ä¸ºMLM+NERæ··åˆä»»åŠ¡"""
    doc = json.load(open(annotation_path, "r", encoding="utf-8"))

    tasks = []
    for clause in doc["clauses"]:
        text = clause["text"]

        # ä»»åŠ¡1: MLM (ä¿æŒæ–‡æœ¬ç†è§£)
        tasks.append({
            "type": "mlm",
            "text": text
        })

        # ä»»åŠ¡2: å®ä½“è¯†åˆ« (æ ‡æ³¨å·¥å…·/ç›®æ ‡/å¯¹è±¡)
        for ann in clause["annotations"]:
            # ç®€åŒ–ä¸ºspanæ ‡æ³¨æ ¼å¼
            entities = []
            for span in ann["evidence_spans"]:
                entities.append({
                    "start": span["start"],
                    "end": span["end"],
                    "label": "INSTRUMENT" if "instrument" in ann else "GOAL"
                })

            tasks.append({
                "type": "ner",
                "text": text,
                "entities": entities
            })

    return tasks
```

**è®­ç»ƒé…ç½®**:
```yaml
tapt_config:
  base_model: "models/dapt_checkpoint"  # ä½¿ç”¨DAPTè¾“å‡º

  task_mix:
    mlm_weight: 0.5
    ner_weight: 0.5

  hyperparameters:
    batch_size: 16
    learning_rate: 2e-5
    epochs: 10
    early_stopping_patience: 3

  validation:
    metric: "NER F1 + MLM Perplexity"
```

**Makefileç›®æ ‡**:
```makefile
tapt:
	@$(ACT) python scripts/prep_tapt_task.py \
		--annotations annotations/adjudicated/*.json \
		--output data/tapt_task.jsonl
	@$(ACT) python scripts/run_tapt.py \
		--model models/dapt_checkpoint \
		--task data/tapt_task.jsonl \
		--output models/tapt_checkpoint \
		--epochs 10
```

---

## ä¸‰ã€RAGæ£€ç´¢å¢å¼ºç”Ÿæˆ

### 3.1 æ£€ç´¢ç´¢å¼•æ„å»º

#### ğŸ“‹ åŒè·¯å¬å›æ¶æ„

```yaml
retrieval_architecture:
  sparse_retrieval:
    engine: "BM25 (Pyserini/Lucene)"
    advantage: "å…³é”®è¯ç²¾ç¡®åŒ¹é…,å¯è§£é‡Šæ€§å¼º"

  dense_retrieval:
    engine: "FAISSå‘é‡æ£€ç´¢"
    model: "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    advantage: "è¯­ä¹‰ç›¸ä¼¼åº¦,å¬å›ç‡é«˜"

  fusion:
    strategy: "åŠ æƒçº¿æ€§ç»„åˆ"
    alpha: 0.5  # BM25æƒé‡
    beta: 0.5   # FAISSæƒé‡
```

#### ğŸ”§ ç´¢å¼•æ„å»ºè„šæœ¬

å·²åœ¨01_æ•°æ®çˆ¬å–æ–¹æ¡ˆä¸­å®Œæˆ,æ­¤å¤„ç›´æ¥å¤ç”¨:

```bash
# ä½¿ç”¨å·²çˆ¬å–çš„æ”¿ç­–æ–‡æœ¬æ„å»ºç´¢å¼•
make index  # æ‰§è¡Œ scripts/build_index.py
```

**ç´¢å¼•éªŒè¯**:
```python
# scripts/validate_index.py
from pyserini.search.lucene import LuceneSearcher

def test_bm25():
    searcher = LuceneSearcher("indexes/bm25")
    hits = searcher.search("äººå·¥æ™ºèƒ½è´¢æ”¿è¡¥è´´", k=10)
    assert len(hits) > 0, "BM25ç´¢å¼•ä¸ºç©º"
    print(f"[BM25] Found {len(hits)} documents")

def test_faiss():
    import faiss
    index = faiss.read_index("indexes/faiss.index")
    assert index.ntotal > 0, "FAISSç´¢å¼•ä¸ºç©º"
    print(f"[FAISS] Indexed {index.ntotal} vectors")

if __name__ == "__main__":
    test_bm25()
    test_faiss()
```

### 3.2 è¯æ®æ£€ç´¢ä¸å¯¹é½

#### ğŸ“‹ æ£€ç´¢æµç¨‹

```python
# scripts/rag_extract.py
import json
from retrieve_evidence import hybrid  # å¤ç”¨01æ–¹æ¡ˆçš„æ··åˆæ£€ç´¢

def retrieve_context(query, k=5):
    """ä¸ºæŠ½å–ä»»åŠ¡æ£€ç´¢ç›¸å…³æ®µè½"""
    results = hybrid(query, k=k, alpha=0.5)

    # åŠ è½½å®Œæ•´æ–‡æ¡£å†…å®¹
    contexts = []
    for doc_id, score in results:
        doc_path = f"corpus/{doc_id}.json"
        doc = json.load(open(doc_path, "r", encoding="utf-8"))
        contexts.append({
            "doc_id": doc_id,
            "title": doc["title"],
            "text": doc["content_text"],
            "score": score,
            "meta": {
                "pub_date": doc["pub_date"],
                "issuer": doc["issuer"],
                "source_url": doc["source_url"]
            }
        })

    return contexts

def format_rag_prompt(query, contexts):
    """æ„é€ RAGæç¤ºè¯"""
    context_str = "\n\n".join([
        f"ã€æ–‡æ¡£{i+1}ã€‘{c['title']}\n"
        f"å‘å¸ƒæœºå…³: {c['meta']['issuer']} | å‘å¸ƒæ—¥æœŸ: {c['meta']['pub_date']}\n"
        f"å†…å®¹æ‘˜è¦: {c['text'][:500]}...\n"
        f"æ¥æº: {c['meta']['source_url']}"
        for i, c in enumerate(contexts)
    ])

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ”¿ç­–åˆ†æä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ£€ç´¢åˆ°çš„æ”¿ç­–æ–‡æ¡£,å›ç­”é—®é¢˜ã€‚

ã€æ£€ç´¢è¯æ®ã€‘
{context_str}

ã€ä»»åŠ¡ã€‘
è¯·ä»ä»¥ä¸Šè¯æ®ä¸­æŠ½å–æ”¿ç­–è¦ç´ ,æŒ‰ä»¥ä¸‹JSON Schemaæ ¼å¼è¾“å‡º:
- goal (ç›®æ ‡/ä»»åŠ¡): æ”¿ç­–è¦è¾¾åˆ°çš„ç›®æ ‡
- instrument (å·¥å…·/æ‰‹æ®µ): å…·ä½“æ”¿ç­–å·¥å…·,å¯å¤šé€‰[funding, tax, land, talent, standard, platform, ip, finance, procurement, pilot, data_compute]
- target_actor (å¯¹è±¡/ä¸»ä½“): æ”¿ç­–é’ˆå¯¹çš„ä¸»ä½“(ä¼ä¸š/é«˜æ ¡/ç§‘ç ”é™¢æ‰€ç­‰)
- region (åœ°åŸŸ/è¦†ç›–): è¡Œæ”¿åŒºåˆ’æˆ–å›­åŒº
- timeframe (æ—¶é—´/æ—¶ç‚¹): ç”Ÿæ•ˆæ—¥æœŸã€æœ‰æ•ˆæœŸ
- strength (å¼ºåº¦/çº¦æŸæ€§): 0=æ— çº¦æŸ, 1=å€¡è®®, 2=ä¸€èˆ¬æ€§, 3=å¼ºçº¦æŸ
- evidence_spans (è¯æ®): åŸæ–‡å¥å­æˆ–è¡Œå·èŒƒå›´

ã€è¦æ±‚ã€‘
1. åªèƒ½ä»æ£€ç´¢è¯æ®ä¸­æŠ½å–,ä¸å¾—ç¼–é€ 
2. ç¼ºä¹è¯æ®æ—¶è¾“å‡ºnull
3. å¿…é¡»æ ‡æ³¨evidence_spansæŒ‡å‘åŸæ–‡ä½ç½®

ã€é—®é¢˜ã€‘
{query}
"""
    return prompt
```

#### ğŸ” è¯æ®æº¯æºæœºåˆ¶

**å¼ºåˆ¶è¦æ±‚**:
- âœ… æ‰€æœ‰æŠ½å–ç»“æœå¿…é¡»é™„å¸¦`evidence_spans`å­—æ®µ
- âœ… `evidence_spans`å¿…é¡»å¯å›æº¯åˆ°å…·ä½“æ–‡æ¡£å’Œæ®µè½
- âœ… ç¼ºä¹è¯æ®æ—¶è¾“å‡º`null`,ç¦æ­¢å¹»è§‰ç”Ÿæˆ

**éªŒè¯è„šæœ¬**:
```python
# scripts/validate_evidence.py
def verify_evidence_span(doc_text, span):
    """éªŒè¯evidence_spanæ˜¯å¦çœŸå®å­˜åœ¨"""
    start, end = span["start"], span["end"]
    assert 0 <= start < end <= len(doc_text), f"Invalid span: {span}"

    extracted_text = doc_text[start:end]
    return extracted_text

def audit_extraction(extraction, original_doc):
    """å®¡è®¡æŠ½å–ç»“æœçš„è¯æ®æœ‰æ•ˆæ€§"""
    for ann in extraction["annotations"]:
        for span in ann["evidence_spans"]:
            text = verify_evidence_span(original_doc["content_text"], span)
            print(f"[Evidence] {text}")
```

---

## å››ã€Few-shot LLMæŠ½å–

### 4.1 æç¤ºè¯å·¥ç¨‹

#### ğŸ“‹ Few-shotç¤ºä¾‹æ„é€ 

```python
# scripts/build_few_shot_examples.py
import json

def load_gold_examples(n=5):
    """ä»é‡‘æ ‡ä¸­é€‰å–é«˜è´¨é‡ç¤ºä¾‹"""
    examples = []

    # é€‰æ‹©æ ‡å‡†: confidence>0.9, strength=2æˆ–3, è¯æ®å®Œæ•´
    gold_files = glob.glob("annotations/adjudicated/*.json")
    for path in gold_files:
        doc = json.load(open(path, "r", encoding="utf-8"))
        for clause in doc["clauses"]:
            for ann in clause["annotations"]:
                if ann["confidence"] >= 0.9 and ann["strength"] >= 2:
                    examples.append({
                        "input": clause["text"],
                        "output": ann
                    })

        if len(examples) >= n:
            break

    return examples[:n]

def format_few_shot_prompt(query, examples):
    """æ„é€ few-shotæç¤ºè¯"""
    example_str = ""
    for i, ex in enumerate(examples):
        example_str += f"\nã€ç¤ºä¾‹{i+1}ã€‘\n"
        example_str += f"è¾“å…¥: {ex['input']}\n"
        example_str += f"è¾“å‡º: {json.dumps(ex['output'], ensure_ascii=False, indent=2)}\n"

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ”¿ç­–è¯­ä¹‰æŠ½å–ä¸“å®¶ã€‚è¯·å‚è€ƒä»¥ä¸‹ç¤ºä¾‹,æŠ½å–æ”¿ç­–è¦ç´ ã€‚

{example_str}

ã€å½“å‰ä»»åŠ¡ã€‘
è¾“å…¥: {query}
è¾“å‡º:
"""
    return prompt
```

### 4.2 æ¨¡å‹è°ƒç”¨ä¸è¾“å‡ºè§£æ

#### ğŸ”§ LLMæ¥å£å°è£…

```python
# scripts/llm_extractor.py
import json, openai

class PolicyExtractor:
    def __init__(self, model="gpt-4", temperature=0.1):
        self.model = model
        self.temperature = temperature
        self.few_shot_examples = load_gold_examples(n=5)

    def extract(self, policy_text, use_rag=True):
        """æ‰§è¡Œæ”¿ç­–è¦ç´ æŠ½å–"""
        # 1. RAGæ£€ç´¢
        if use_rag:
            contexts = retrieve_context(policy_text, k=5)
            prompt = format_rag_prompt(policy_text, contexts)
        else:
            prompt = format_few_shot_prompt(policy_text, self.few_shot_examples)

        # 2. LLMè°ƒç”¨
        response = openai.ChatCompletion.create(
            model=self.model,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯æ”¿ç­–åˆ†æä¸“å®¶,ä¸¥æ ¼æŒ‰JSON Schemaè¾“å‡ºã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=self.temperature,
            max_tokens=2000
        )

        # 3. è§£æè¾“å‡º
        raw_output = response.choices[0].message.content
        try:
            parsed = json.loads(raw_output)
            return self.post_process(parsed)
        except json.JSONDecodeError as e:
            print(f"[ERROR] JSONè§£æå¤±è´¥: {e}")
            return None

    def post_process(self, extraction):
        """åå¤„ç†: è§„èŒƒåŒ–æšä¸¾å€¼ã€éªŒè¯å¿…é¡»å­—æ®µ"""
        # å·¥å…·ç±»å‹è§„èŒƒåŒ–
        if "instrument" in extraction:
            valid_instruments = ["funding","tax","land","talent","standard",
                                 "platform","ip","finance","procurement","pilot","data_compute"]
            extraction["instrument"] = [
                i for i in extraction["instrument"] if i in valid_instruments
            ]

        # å¼ºåº¦èŒƒå›´æ£€æŸ¥
        if "strength" in extraction:
            extraction["strength"] = max(0, min(3, extraction["strength"]))

        return extraction
```

### 4.3 æ‰¹é‡å¤„ç†æµç¨‹

**Makefileç›®æ ‡**:
```makefile
extract:
	@echo "[Extract] Processing corpus..."
	@$(ACT) python scripts/batch_extract.py \
		--corpus corpus/*.json \
		--output extractions/ \
		--model gpt-4 \
		--use_rag true \
		--batch_size 10
```

**æ‰¹é‡è„šæœ¬**:
```python
# scripts/batch_extract.py
import glob, json
from tqdm import tqdm
from llm_extractor import PolicyExtractor

def main():
    extractor = PolicyExtractor(model="gpt-4", temperature=0.1)

    corpus_files = glob.glob("corpus/*.json")
    for path in tqdm(corpus_files):
        doc = json.load(open(path, "r", encoding="utf-8"))

        # å¯¹æ¯ä¸ªæ–‡æ¡£çš„æ¯ä¸ªæ¡æ¬¾æ‰§è¡ŒæŠ½å–
        results = []
        for clause in doc.get("clauses", [{"text": doc["content_text"]}]):
            extraction = extractor.extract(clause["text"], use_rag=True)
            if extraction:
                results.append(extraction)

        # ä¿å­˜ç»“æœ
        output_path = f"extractions/{doc['doc_id']}.json"
        json.dump(results, open(output_path, "w", encoding="utf-8"),
                  ensure_ascii=False, indent=2)

if __name__ == "__main__":
    main()
```

---

## äº”ã€JSON SchemaéªŒè¯

### 5.1 Schemaè§„èŒƒ

**å®Œæ•´Schemaå®šä¹‰** (å¤ç”¨é¡¹ç›®æ–¹æ¡ˆç»†èŠ‚.txtä¸­çš„å®šä¹‰):

```json
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://psc-graph.org/policy-schema.json",
  "title": "Policy Semantic Causal Graph Annotation",
  "type": "object",
  "required": ["doc_id", "title", "issuer", "pub_date", "clauses"],
  "properties": {
    "doc_id": {"type": "string"},
    "title": {"type": "string"},
    "issuer": {"type": "string"},
    "pub_date": {"type": "string", "format": "date"},
    "url": {"type": "string"},
    "region_scope": {
      "type": "object",
      "properties": {
        "name": {"type": "string"},
        "admin_code": {"type": ["string", "null"]},
        "uncertain": {"type": "boolean", "default": false}
      }
    },
    "clauses": {
      "type": "array",
      "items": {
        "type": "object",
        "required": ["clause_id", "text", "annotations"],
        "properties": {
          "clause_id": {"type": "string"},
          "text": {"type": "string"},
          "span_start": {"type": "integer"},
          "span_end": {"type": "integer"},
          "annotations": {
            "type": "array",
            "items": {
              "type": "object",
              "required": ["goal", "instrument", "target_actor", "strength", "evidence_spans", "confidence"],
              "properties": {
                "goal": {"type": "string"},
                "instrument": {
                  "type": "array",
                  "items": {
                    "type": "string",
                    "enum": ["funding","tax","land","talent","standard","platform","ip","finance","procurement","pilot","data_compute","other"]
                  }
                },
                "target_actor": {"type": "string"},
                "region": {
                  "type": "object",
                  "properties": {
                    "name": {"type": "string"},
                    "admin_code": {"type": ["string","null"]},
                    "uncertain": {"type": "boolean", "default": false}
                  }
                },
                "timeframe": {
                  "type": "object",
                  "properties": {
                    "effective_date": {"type": ["string","null"], "format":"date"},
                    "expiry_date": {"type": ["string","null"], "format":"date"}
                  }
                },
                "strength": {"type": "integer", "minimum": 0, "maximum": 3},
                "support": {
                  "type": "array",
                  "items": {
                    "type": "object",
                    "properties": {
                      "type": {"type": "string", "enum": ["funding","tax","quota","land","fast_track","other"]},
                      "value": {"type": ["number","null"]},
                      "unit": {"type": ["string","null"]}
                    }
                  }
                },
                "evidence_spans": {
                  "type": "array",
                  "items": {
                    "type": "object",
                    "required": ["start","end"],
                    "properties": {
                      "start": {"type": "integer"},
                      "end": {"type": "integer"}
                    }
                  }
                },
                "confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0}
              }
            }
          }
        }
      }
    }
  }
}
```

### 5.2 éªŒè¯è„šæœ¬

```python
# scripts/validate_extractions.py
import json, glob
from jsonschema import validate, Draft202012Validator

SCHEMA_PATH = "schemas/policy_schema.json"

def load_schema():
    with open(SCHEMA_PATH, "r", encoding="utf-8") as f:
        return json.load(f)

def validate_extraction(extraction, schema):
    """éªŒè¯å•ä¸ªæŠ½å–ç»“æœ"""
    errors = sorted(
        Draft202012Validator(schema).iter_errors(extraction),
        key=lambda e: e.path
    )
    return [str(e) for e in errors]

def main():
    schema = load_schema()
    extraction_files = glob.glob("extractions/*.json")

    total, valid, invalid = 0, 0, 0
    for path in extraction_files:
        extractions = json.load(open(path, "r", encoding="utf-8"))
        for ext in extractions:
            total += 1
            errors = validate_extraction(ext, schema)
            if errors:
                invalid += 1
                print(f"[INVALID] {path}")
                for e in errors:
                    print(f"  - {e}")
            else:
                valid += 1

    print(f"\n[Summary] Total: {total}, Valid: {valid}, Invalid: {invalid}")
    print(f"[Pass Rate] {valid/total*100:.2f}%")

if __name__ == "__main__":
    main()
```

**Makefileç›®æ ‡**:
```makefile
validate_extract:
	@$(ACT) python scripts/validate_extractions.py
```

---

## å…­ã€ä¸ç¡®å®šæ€§é‡åŒ–

### 6.1 æ¸©åº¦ç¼©æ”¾(Temperature Scaling)

#### ğŸ“‹ ç›®æ ‡

æ ¡å‡†LLMè¾“å‡ºçš„ç½®ä¿¡åº¦,ä½¿å…¶åæ˜ çœŸå®å‡†ç¡®ç‡ã€‚

**ç†è®ºåŸºç¡€**:
- Guo et al., ICML 2017: "On Calibration of Modern Neural Networks"
- æ–¹æ³•: åœ¨éªŒè¯é›†ä¸Šå­¦ä¹ æ¸©åº¦å‚æ•°T,ä½¿æ ¡å‡†è¯¯å·®(ECE)æœ€å°

#### ğŸ”§ å®ç°è„šæœ¬

```python
# scripts/temperature_scaling.py
import numpy as np
from sklearn.metrics import log_loss

class TemperatureScaler:
    def __init__(self):
        self.T = 1.0

    def fit(self, logits, labels):
        """åœ¨éªŒè¯é›†ä¸Šæ‹Ÿåˆæ¸©åº¦å‚æ•°"""
        def softmax(x, t):
            e = np.exp(x / t - (x / t).max(axis=1, keepdims=True))
            return e / e.sum(1, keepdims=True)

        # ç½‘æ ¼æœç´¢æœ€ä¼˜æ¸©åº¦
        Ts = np.linspace(0.5, 5.0, 50)
        best_nll, best_t = float('inf'), 1.0

        for t in Ts:
            probs = softmax(logits, t)
            nll = log_loss(labels, probs)
            if nll < best_nll:
                best_nll, best_t = nll, t

        self.T = best_t
        print(f"[TempScale] Optimal T={best_t:.3f}, NLL={best_nll:.3f}")
        return self

    def predict_proba(self, logits):
        """è¿”å›æ ¡å‡†åçš„æ¦‚ç‡"""
        e = np.exp(logits / self.T - (logits / self.T).max(axis=1, keepdims=True))
        return e / e.sum(1, keepdims=True)

def expected_calibration_error(probs, labels, M=15):
    """è®¡ç®—æœŸæœ›æ ¡å‡†è¯¯å·®(ECE)"""
    bins = np.linspace(0, 1, M+1)
    ece = 0
    conf = probs.max(1)
    preds = probs.argmax(1)

    for i in range(M):
        idx = (conf > bins[i]) & (conf <= bins[i+1])
        if idx.sum() == 0:
            continue
        acc = (preds[idx] == labels[idx]).mean()
        avg_conf = conf[idx].mean()
        ece += (idx.mean()) * abs(acc - avg_conf)

    return ece

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # logits: ä»æŠ½å–æ¨¡å‹è·å–çš„åŸå§‹åˆ†æ•° (n_samples, n_classes)
    # labels: çœŸå®æ ‡ç­¾ (n_samples,)
    logits_val = np.load("data/validation_logits.npy")
    labels_val = np.load("data/validation_labels.npy")

    scaler = TemperatureScaler().fit(logits_val, labels_val)
    probs_cal = scaler.predict_proba(logits_val)

    ece = expected_calibration_error(probs_cal, labels_val)
    print(f"[ECE] {ece:.3f} (ç›®æ ‡ â‰¤ 0.05)")
```

### 6.2 å…±å½¢é¢„æµ‹(Conformal Prediction)

#### ğŸ“‹ ç›®æ ‡

æä¾›åˆ†å¸ƒæ— å…³çš„è¦†ç›–ä¿è¯,è¾“å‡ºå¯èƒ½æ ‡ç­¾é›†åˆè€Œéå•ä¸€é¢„æµ‹ã€‚

**ç†è®ºåŸºç¡€**:
- Angelopoulos & Bates: "A Gentle Introduction to Conformal Prediction"
- ä¿è¯: åœ¨(1-Î±)ç½®ä¿¡æ°´å¹³ä¸‹,çœŸå®æ ‡ç­¾åŒ…å«åœ¨é¢„æµ‹é›†ä¸­

#### ğŸ”§ å®ç°è„šæœ¬

```python
# scripts/conformal_prediction.py
import numpy as np

def conformal_prediction(probs_calib, y_calib, probs_test, alpha=0.1):
    """
    åŸºäºæ ¡å‡†é›†çš„å…±å½¢é¢„æµ‹

    å‚æ•°:
        probs_calib: æ ¡å‡†é›†æ¦‚ç‡ (n_calib, n_classes)
        y_calib: æ ¡å‡†é›†æ ‡ç­¾ (n_calib,)
        probs_test: æµ‹è¯•é›†æ¦‚ç‡ (n_test, n_classes)
        alpha: æ˜¾è‘—æ€§æ°´å¹³(é»˜è®¤0.1,å³90%è¦†ç›–ç‡)

    è¿”å›:
        prediction_sets: boolæ•°ç»„ (n_test, n_classes)
        tau: é˜ˆå€¼
    """
    # è®¡ç®—æ ¡å‡†é›†ä¸åˆæ ¼åˆ†æ•°: s = 1 - p_true
    scores_calib = 1 - probs_calib[np.arange(len(y_calib)), y_calib]

    # è®¡ç®—åˆ†ä½æ•°é˜ˆå€¼
    n = len(scores_calib)
    k = int(np.ceil((1 - alpha) * (n + 1))) - 1
    k = max(min(k, n-1), 0)
    tau = np.sort(scores_calib)[k]

    # é¢„æµ‹é›†: {c | 1 - p_c <= tau} â‰¡ {c | p_c >= 1 - tau}
    prediction_sets = (probs_test >= (1 - tau))

    return prediction_sets, tau

def evaluate_coverage(pred_sets, y_test):
    """è¯„ä¼°è¦†ç›–ç‡ä¸å¹³å‡é›†åˆå¤§å°"""
    coverage = np.mean([y_test[i] in np.where(pred_sets[i])[0]
                        for i in range(len(y_test))])
    avg_size = pred_sets.sum(1).mean()

    return coverage, avg_size

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åŠ è½½æ ¡å‡†åçš„æ¦‚ç‡
    probs_cal = np.load("data/calibrated_probs_val.npy")
    y_cal = np.load("data/validation_labels.npy")
    probs_test = np.load("data/calibrated_probs_test.npy")
    y_test = np.load("data/test_labels.npy")

    # æ‰§è¡Œå…±å½¢é¢„æµ‹
    pred_sets, tau = conformal_prediction(probs_cal, y_cal, probs_test, alpha=0.1)

    # è¯„ä¼°
    coverage, avg_size = evaluate_coverage(pred_sets, y_test)
    print(f"[Conformal] Coverage={coverage:.3f} (ç›®æ ‡â‰¥0.90), Avg Set Size={avg_size:.2f}")
    print(f"[Threshold] tau={tau:.3f}")
```

### 6.3 é«˜é£é™©å†³ç­–æ ‡è®°

```python
# scripts/flag_high_risk.py
def flag_uncertain_extractions(extraction, probs, tau):
    """æ ‡è®°ä¸ç¡®å®šæ€§é«˜çš„æŠ½å–ç»“æœ"""
    # å¦‚æœæ¦‚ç‡ä½äºé˜ˆå€¼,æ ‡è®°ä¸ºéœ€è¦äººå·¥å¤æ ¸
    if probs.max() < (1 - tau):
        extraction["requires_review"] = True
        extraction["review_reason"] = "Low confidence (conformal prediction)"

    # å¦‚æœstrength=3(å¼ºçº¦æŸ)ä¸”ç½®ä¿¡åº¦<0.8,ä¹Ÿæ ‡è®°
    if extraction.get("strength") == 3 and extraction.get("confidence", 0) < 0.8:
        extraction["requires_review"] = True
        extraction["review_reason"] = "High-stakes classification with low confidence"

    return extraction
```

---

## ä¸ƒã€è´¨é‡ä¿è¯ä¸è¯„æµ‹

### 7.1 è¯„æµ‹æŒ‡æ ‡

| æŒ‡æ ‡ç±»åˆ« | å…·ä½“æŒ‡æ ‡ | ç›®æ ‡å€¼ | è®¡ç®—æ–¹æ³• |
|---------|---------|--------|----------|
| **æŠ½å–è´¨é‡** | å®ä½“P/R/F1 | F1â‰¥0.85 | ä¸é‡‘æ ‡å¯¹æ¯” |
| | å…³ç³»F1 | F1â‰¥0.80 | äº”å…ƒç»„å®Œæ•´åŒ¹é… |
| **ä¸€è‡´æ€§** | Cohen's Îº | Îºâ‰¥0.80 | åŒäººæ ‡æ³¨ä¸€è‡´æ€§ |
| **æ ¡å‡†** | ECE | â‰¤0.05 | æœŸæœ›æ ¡å‡†è¯¯å·® |
| **è¯æ®è´¨é‡** | è¯æ®å‘½ä¸­ç‡ | â‰¥0.90 | è¢«æ£€ç´¢åˆ°çš„æ¯”ä¾‹ |
| **RAGæ•ˆæœ** | ä¸Šä¸‹æ–‡ç›¸å…³æ€§ | â‰¥0.85 | ARESè¯„æµ‹ |
| | å¿ å®åº¦ | â‰¥0.90 | æŠ½å–æ¥æºå¯éªŒè¯ |

### 7.2 æ¶ˆèå®éªŒ

**å¯¹ç…§ç»„è®¾è®¡**:
```yaml
ablation_groups:
  baseline:
    name: "çº¯é›¶æ ·æœ¬LLM"
    config: "æ— DAPT/TAPT,æ— RAG,çº¯GPT-4 zero-shot"

  ablation_1:
    name: "ä»…DAPT"
    config: "æœ‰DAPT,æ— TAPT,æ— RAG"

  ablation_2:
    name: "DAPT+TAPT"
    config: "æœ‰DAPT+TAPT,æ— RAG"

  ablation_3:
    name: "ä»…RAG"
    config: "æ— DAPT/TAPT,æœ‰RAG(BM25+FAISS)"

  full_model:
    name: "å®Œæ•´ç³»ç»Ÿ"
    config: "DAPT+TAPT+RAG+æ¸©åº¦ç¼©æ”¾+å…±å½¢é¢„æµ‹"
```

**æ‰§è¡Œè„šæœ¬**:
```bash
# scripts/run_ablation.sh
#!/bin/bash

for group in baseline ablation_1 ablation_2 ablation_3 full_model; do
    echo "[Ablation] Running $group..."
    python scripts/batch_extract.py \
        --config configs/${group}.yaml \
        --output results/ablation/${group}/

    python scripts/evaluate.py \
        --predictions results/ablation/${group}/ \
        --gold annotations/adjudicated/ \
        --output results/ablation/${group}_metrics.json
done

# æ±‡æ€»å¯¹æ¯”
python scripts/compare_ablation.py --results results/ablation/
```

### 7.3 ARESè‡ªåŠ¨åŒ–è¯„æµ‹

**é…ç½®æ–‡ä»¶**:
```yaml
# configs/ares_eval.yaml
ares_config:
  metrics:
    - context_relevance  # æ£€ç´¢ä¸Šä¸‹æ–‡ç›¸å…³æ€§
    - answer_faithfulness  # ç­”æ¡ˆå¿ å®åº¦
    - answer_relevance  # ç­”æ¡ˆç›¸å…³æ€§

  thresholds:
    context_relevance: 0.85
    answer_faithfulness: 0.90
    answer_relevance: 0.88

  evaluation_set: "data/ares_eval_set.json"
```

**å‚è€ƒæ–‡çŒ®**:
- ARES (arXiv:2311.09476): "An Automated Evaluation Framework for Retrieval-Augmented Generation Systems"

---

## å…«ã€å®‰å…¨ä¸åˆè§„

### 8.1 OWASP LLM Top 10åˆè§„

**é˜²æŠ¤æªæ–½**:

| é£é™©é¡¹ | é˜²æŠ¤æªæ–½ | å®ç°ä½ç½® |
|-------|---------|----------|
| **Prompt Injection** | è¾“å…¥è¿‡æ»¤ã€æ¨¡æ¿åŒ–æç¤ºè¯ | `scripts/llm_extractor.py` |
| **Excessive Agency** | æœ€å°æƒé™ã€æ²™ç®±æ‰§è¡Œ | å·¥å…·è°ƒç”¨é™åˆ¶ |
| **Data Leakage** | ä»…ä½¿ç”¨å…¬å¼€æ”¿ç­–æ–‡æœ¬ | æ•°æ®æºå®¡è®¡ |
| **Output Hallucination** | å¼ºåˆ¶è¯æ®æº¯æºã€RAGçº¦æŸ | `evidence_spans`éªŒè¯ |

**å®¡è®¡æ—¥å¿—**:
```python
# scripts/security_audit.py
import logging

logging.basicConfig(
    filename="logs/security_audit.log",
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s'
)

def log_extraction(doc_id, user, action, result):
    logging.info(f"DOC={doc_id} | USER={user} | ACTION={action} | RESULT={result}")
```

### 8.2 æ•°æ®åˆè§„å£°æ˜

```yaml
data_compliance:
  sources:
    - "å›½åŠ¡é™¢æ”¿ç­–æ–‡ä»¶åº“(å…¬å¼€)"
    - "çœçº§ç§‘æŠ€å…å®˜ç½‘(å…¬å¼€)"
    - "å›½å®¶ç»Ÿè®¡å±€æ•°æ®(å…¬å¼€)"

  no_personal_data: true
  no_sensitive_info: true

  citation_required: true
  source_url_preserved: true
```

---

## ä¹ã€å®æ–½æ­¥éª¤

### 9.1 ç¯å¢ƒå‡†å¤‡

**Day 1-2: ç¯å¢ƒæ­å»º**

```bash
# 1. åˆ›å»ºPythonè™šæ‹Ÿç¯å¢ƒ
python3 -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# 2. å®‰è£…ä¾èµ–
pip install -r scripts/requirements.txt

# ç‰¹æ®Šä¾èµ–
pip install sentence-transformers==3.0.1
pip install pyserini==0.40.0
pip install faiss-cpu==1.8.0

# 3. éªŒè¯å®‰è£…
python scripts/validate_env.py
```

### 9.2 æ•°æ®å‡†å¤‡

**Day 3: DAPTè¯­æ–™å‡†å¤‡**

```bash
# ä½¿ç”¨å·²çˆ¬å–çš„æ”¿ç­–æ–‡æœ¬
make dapt_corpus  # æ‰§è¡Œ scripts/prep_dapt_corpus.py

# æ£€æŸ¥è¯­æ–™ç»Ÿè®¡
python scripts/corpus_stats.py
# é¢„æœŸè¾“å‡º: 500ä¸‡-1000ä¸‡å­—,æ—¶é—´è·¨åº¦1986-è‡³ä»Š
```

**Day 4-5: é‡‘æ ‡æ ‡æ³¨å‡†å¤‡**

```bash
# ä»å·²çˆ¬å–æ–‡æœ¬ä¸­æŠ½æ ·500-1000æ¡
python scripts/sample_for_annotation.py \
    --corpus corpus/*.json \
    --output annotations/to_annotate/ \
    --n 1000 \
    --strategy stratified  # æŒ‰æ—¶é—´/å‘æ–‡å•ä½åˆ†å±‚æŠ½æ ·

# æ³¨: å®é™…æ ‡æ³¨æµç¨‹è§ 03_æ ‡æ³¨ä¸è¯„ä¼°æ–¹æ¡ˆ.md
```

### 9.3 æ¨¡å‹è®­ç»ƒ

**Day 6-8: DAPTè®­ç»ƒ**

```bash
# å¯åŠ¨DAPTè®­ç»ƒ
make dapt

# ç›‘æ§è®­ç»ƒ
tensorboard --logdir models/dapt_checkpoint/logs

# é¢„æœŸæ—¶é—´: 2-3å¤©(1x A100 40GB)
```

**Day 9-10: TAPTè®­ç»ƒ**

```bash
# éœ€è¦å…ˆå®Œæˆé‡‘æ ‡æ ‡æ³¨(è§03æ–¹æ¡ˆ)
make tapt

# é¢„æœŸæ—¶é—´: 1-2å¤©
```

### 9.4 ç´¢å¼•ä¸æ£€ç´¢

**Day 11: æ„å»ºæ£€ç´¢ç´¢å¼•**

```bash
# å¤ç”¨01æ–¹æ¡ˆçš„ç´¢å¼•æ„å»º
make index

# éªŒè¯ç´¢å¼•
python scripts/validate_index.py
```

### 9.5 æŠ½å–ä¸éªŒè¯

**Day 12-13: æ‰¹é‡æŠ½å–**

```bash
# æ‰§è¡Œæ‰¹é‡æŠ½å–
make extract

# é¢„æœŸå¤„ç†é€Ÿåº¦: 100-200æ¡/å°æ—¶(GPT-4 API)
```

**Day 14: è´¨é‡éªŒè¯**

```bash
# JSON SchemaéªŒè¯
make validate_extract

# è®¡ç®—è¯„æµ‹æŒ‡æ ‡
python scripts/evaluate.py \
    --predictions extractions/ \
    --gold annotations/adjudicated/ \
    --output results/extraction_metrics.json

# è¿è¡Œæ¶ˆèå®éªŒ
bash scripts/run_ablation.sh
```

---

## åã€å¸¸è§é—®é¢˜ä¸æ•…éšœæ’é™¤

### Q1: DAPTè®­ç»ƒæ—¶GPUå†…å­˜ä¸è¶³

**ç°è±¡**: CUDA OOMé”™è¯¯

**è§£å†³æ–¹æ¡ˆ**:
```yaml
solutions:
  option_1: "å‡å°batch_size (32â†’16â†’8)"
  option_2: "ä½¿ç”¨gradient_accumulation_steps=4"
  option_3: "ä½¿ç”¨fp16æ··åˆç²¾åº¦è®­ç»ƒ"
  option_4: "ä½¿ç”¨æ¨¡å‹å¹¶è¡Œ(DeepSpeed ZeRO-2/3)"
```

### Q2: BM25ç´¢å¼•æ„å»ºå¤±è´¥

**ç°è±¡**: PyseriniæŠ¥é”™"Java heap space"

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å¢åŠ Javaå †å†…å­˜
export JAVA_OPTS="-Xmx8g"

# æˆ–åˆ†æ‰¹ç´¢å¼•
python scripts/build_index_batch.py --batch_size 1000
```

### Q3: RAGæ£€ç´¢å¬å›ç‡ä½

**ç°è±¡**: ç›¸å…³æ–‡æ¡£æœªè¢«æ£€ç´¢åˆ°

**æ’æŸ¥æ­¥éª¤**:
```python
# 1. æ£€æŸ¥æŸ¥è¯¢è¯åˆ†è¯
from pyserini.analysis import Analyzer
analyzer = Analyzer(Analyzer.CHINESE)
tokens = analyzer.analyze("äººå·¥æ™ºèƒ½è´¢æ”¿è¡¥è´´")
print(tokens)  # åº”æ­£ç¡®åˆ†è¯

# 2. è°ƒæ•´BM25å‚æ•°
searcher = LuceneSearcher("indexes/bm25")
searcher.set_bm25(k1=1.2, b=0.75)  # é»˜è®¤k1=0.9, b=0.4

# 3. å¢åŠ å‘é‡æ£€ç´¢æƒé‡
alpha = 0.3  # BM25æƒé‡é™ä½
beta = 0.7   # FAISSæƒé‡æé«˜
```

### Q4: LLMæŠ½å–JSONæ ¼å¼ä¸è§„èŒƒ

**ç°è±¡**: è¾“å‡ºä¸ç¬¦åˆJSON Schema

**è§£å†³æ–¹æ¡ˆ**:
```python
# åœ¨æç¤ºè¯ä¸­å¢åŠ æ ¼å¼çº¦æŸ
prompt += """
ã€è¾“å‡ºæ ¼å¼è¦æ±‚ã€‘
1. ä¸¥æ ¼æŒ‰JSONæ ¼å¼è¾“å‡º,ä¸å¾—åŒ…å«å…¶ä»–æ–‡å­—
2. æ‰€æœ‰å­—æ®µåå¿…é¡»ç”¨åŒå¼•å·
3. instrumentå¿…é¡»ä»æšä¸¾å€¼ä¸­é€‰æ‹©
4. strengthå¿…é¡»ä¸º0-3çš„æ•´æ•°
5. evidence_spansçš„start/endå¿…é¡»ä¸ºæ•´æ•°

ã€é”™è¯¯ç¤ºä¾‹ã€‘
âŒ {goal: 'ä¿ƒè¿›åˆ›æ–°'}  // å­—æ®µåæœªåŠ å¼•å·
âŒ {strength: 'å¼º'}    // åº”ä¸ºæ•´æ•°è€Œéå­—ç¬¦ä¸²

ã€æ­£ç¡®ç¤ºä¾‹ã€‘
âœ… {"goal": "ä¿ƒè¿›åˆ›æ–°", "strength": 3}
"""
```

### Q5: æ¸©åº¦ç¼©æ”¾åECEä»ç„¶è¾ƒé«˜

**ç°è±¡**: ECE > 0.05

**æ’æŸ¥æ­¥éª¤**:
```python
# 1. æ£€æŸ¥éªŒè¯é›†å¤§å°(å»ºè®®â‰¥500æ¡)
print(f"Validation set size: {len(logits_val)}")

# 2. ç»˜åˆ¶å¯é æ€§å›¾(Reliability Diagram)
import matplotlib.pyplot as plt

def plot_reliability_diagram(probs, labels, M=15):
    bins = np.linspace(0, 1, M+1)
    conf = probs.max(1)
    preds = probs.argmax(1)

    bin_accs, bin_confs = [], []
    for i in range(M):
        idx = (conf > bins[i]) & (conf <= bins[i+1])
        if idx.sum() > 0:
            bin_accs.append((preds[idx] == labels[idx]).mean())
            bin_confs.append(conf[idx].mean())

    plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')
    plt.plot(bin_confs, bin_accs, 'ro-', label='Model')
    plt.xlabel('Confidence'); plt.ylabel('Accuracy')
    plt.legend(); plt.savefig('reliability_diagram.png')

plot_reliability_diagram(probs_cal, labels_val)

# 3. è€ƒè™‘åˆ†å±‚æ ¡å‡†(æŒ‰å¼ºåº¦/å·¥å…·ç±»å‹åˆ†ç»„)
for strength in [0, 1, 2, 3]:
    idx = (labels_val_strength == strength)
    scaler = TemperatureScaler().fit(logits_val[idx], labels_val[idx])
```

### Q6: å…±å½¢é¢„æµ‹é›†åˆè¿‡å¤§

**ç°è±¡**: å¹³å‡é›†åˆå¤§å°>5,ä¸å¤Ÿinformative

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. é™ä½alpha(æé«˜è¦†ç›–è¦æ±‚)
alpha = 0.05  # ä»0.1é™ä½åˆ°0.05

# 2. ä½¿ç”¨è‡ªé€‚åº”å…±å½¢é¢„æµ‹(APS)
def adaptive_conformal_prediction(probs_calib, y_calib, probs_test, alpha=0.1):
    """ä½¿ç”¨ç´¯ç§¯æ¦‚ç‡æ’åº"""
    # å¯¹æ¯ä¸ªæ ·æœ¬,æŒ‰æ¦‚ç‡ä»å¤§åˆ°å°ç´¯åŠ ,ç›´åˆ°è¶…è¿‡é˜ˆå€¼
    # å‚è€ƒ: Romano et al., 2020 "Classification with Valid and Adaptive Coverage"
    pass  # å®ç°ç•¥

# 3. æå‡æ¨¡å‹æœ¬èº«çš„ç½®ä¿¡åº¦(æ”¹è¿›DAPT/TAPT/few-shot)
```

### Q7: è¯æ®æº¯æºå¤±è´¥

**ç°è±¡**: evidence_spansæŒ‡å‘çš„æ–‡æœ¬ä¸æŠ½å–å†…å®¹ä¸åŒ¹é…

**è§£å†³æ–¹æ¡ˆ**:
```python
# åœ¨åå¤„ç†ä¸­å¼ºåˆ¶éªŒè¯
def enforce_evidence_alignment(extraction, original_text):
    for ann in extraction["annotations"]:
        # æ£€æŸ¥æ¯ä¸ªevidence_span
        for span in ann["evidence_spans"]:
            evidence_text = original_text[span["start"]:span["end"]]

            # éªŒè¯è¯æ®æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«æŠ½å–çš„å…³é”®è¯
            goal_keywords = ann["goal"].split()[:3]
            if not any(kw in evidence_text for kw in goal_keywords):
                # æ ‡è®°ä¸ºéœ€è¦äººå·¥å¤æ ¸
                ann["requires_review"] = True
                ann["review_reason"] = "Evidence-extraction mismatch"

    return extraction
```

---

## åä¸€ã€éªŒæ”¶æ ‡å‡†

### 11.1 åŠŸèƒ½å®Œæ•´æ€§

| åŠŸèƒ½é¡¹ | éªŒæ”¶æ ‡å‡† | éªŒè¯æ–¹æ³• |
|-------|---------|----------|
| DAPTæ¨¡å‹ | åœ¨æ”¿ç­–é¢†åŸŸå›°æƒ‘åº¦ < baselineæ¨¡å‹ | éªŒè¯é›†perplexityå¯¹æ¯” |
| TAPTæ¨¡å‹ | NER F1 > DAPTæ¨¡å‹ | æµ‹è¯•é›†F1å¯¹æ¯” |
| RAGç´¢å¼• | BM25+FAISSç´¢å¼•æˆåŠŸæ„å»º | `python scripts/validate_index.py` |
| æŠ½å–ç³»ç»Ÿ | æ‰¹é‡å¤„ç†â‰¥1000æ¡æ”¿ç­–æ–‡æœ¬ | æ£€æŸ¥extractions/æ–‡ä»¶æ•° |
| SchemaéªŒè¯ | 100%è¾“å‡ºç¬¦åˆJSON Schema | `make validate_extract`é€šè¿‡ç‡=100% |

### 11.2 è´¨é‡æŒ‡æ ‡

```yaml
quality_metrics:
  extraction_f1:
    entity_f1: "â‰¥0.85"
    relation_f1: "â‰¥0.80"

  consistency:
    cohen_kappa: "â‰¥0.80"

  calibration:
    ece: "â‰¤0.05"

  rag_quality:
    context_relevance: "â‰¥0.85"
    answer_faithfulness: "â‰¥0.90"
    answer_relevance: "â‰¥0.88"

  evidence_quality:
    evidence_hit_rate: "â‰¥0.90"
```

### 11.3 æ€§èƒ½æŒ‡æ ‡

```yaml
performance_metrics:
  throughput:
    extraction_speed: "â‰¥100æ¡/å°æ—¶"

  resource_usage:
    gpu_memory: "â‰¤40GB (å•å¡A100)"
    cpu_memory: "â‰¤64GB"

  latency:
    single_extraction: "â‰¤60ç§’"
```

### 11.4 äººå·¥æŠ½æŸ¥

**æŠ½æŸ¥æ¸…å•**:
```yaml
manual_review:
  sample_size: 50
  sampling_strategy: "åˆ†å±‚æŠ½æ ·(æŒ‰å¼ºåº¦/å·¥å…·ç±»å‹)"

  review_items:
    - äº”å…ƒç»„å®Œæ•´æ€§
    - evidence_spansæœ‰æ•ˆæ€§
    - å·¥å…·ç±»å‹å‡†ç¡®æ€§
    - å¼ºåº¦åˆ†çº§åˆç†æ€§
    - æ—¶é—´ä¿¡æ¯æ­£ç¡®æ€§

  acceptance:
    accuracy: "â‰¥95%"
    major_errors: "=0"  # å¼ºåº¦é”™æ ‡ã€å·¥å…·ç±»å‹å®Œå…¨é”™è¯¯
```

---

## åäºŒã€æ—¶é—´è¡¨ä¸é‡Œç¨‹ç¢‘

```yaml
week_3:
  day_1_2:
    - âœ… ç¯å¢ƒæ­å»ºä¸ä¾èµ–å®‰è£…
    - âœ… DAPTè¯­æ–™å‡†å¤‡
  day_3:
    - âœ… é‡‘æ ‡æŠ½æ ·ä¸æ ‡æ³¨å¯åŠ¨
  day_4_5:
    - âœ… DAPTè®­ç»ƒå¯åŠ¨
    - ğŸ”„ æŒç»­ç›‘æ§è®­ç»ƒè¿›åº¦

week_4:
  day_6_7:
    - âœ… DAPTè®­ç»ƒå®Œæˆ
    - âœ… TAPTæ•°æ®å‡†å¤‡ä¸è®­ç»ƒå¯åŠ¨
  day_8_9:
    - âœ… TAPTè®­ç»ƒå®Œæˆ
    - âœ… RAGç´¢å¼•æ„å»º
  day_10:
    - âœ… æ‰¹é‡æŠ½å–æ‰§è¡Œ
    - âœ… è´¨é‡éªŒè¯ä¸æ¶ˆèå®éªŒ
```

---

## åä¸‰ã€é™„å½•

### é™„å½•A: æ”¿ç­–ä¸“ä¸šæœ¯è¯­è¯è¡¨

```yaml
policy_instruments:
  financial:
    - è´¢æ”¿è¡¥è´´, ä¸“é¡¹èµ„é‡‘, èµ„é‡‘æ”¯æŒ, ç»è´¹æ‹¨ä»˜
    - ç¨æ”¶ä¼˜æƒ , å‡å…ç¨, ç¨æ”¶æŠµæ‰£, æ‰€å¾—ç¨ä¼˜æƒ 

  land_resources:
    - ç”¨åœ°æ”¯æŒ, åœŸåœ°ä¾›åº”, äº§ä¸šç”¨åœ°, åœŸåœ°ä¼˜æƒ 
    - èƒ½è€—æŒ‡æ ‡, æ’æ”¾é¢åº¦, ç¢³æ’æ”¾æƒ

  talent:
    - äººæ‰å¼•è¿›, äººæ‰è¡¥è´´, ä½æˆ¿è¡¥è´´, è½æˆ·æ”¿ç­–
    - èŒç§°è¯„å®¡, äººæ‰è¯„ä»·, ä¸“å®¶æ´¥è´´

  platform:
    - åˆ›æ–°å¹³å°, å­µåŒ–å™¨, åŠ é€Ÿå™¨, äº§ä¸šå›­åŒº
    - ç ”å‘ä¸­å¿ƒ, é‡ç‚¹å®éªŒå®¤, å·¥ç¨‹ç ”ç©¶ä¸­å¿ƒ

  ip:
    - çŸ¥è¯†äº§æƒä¿æŠ¤, ä¸“åˆ©èµ„åŠ©, å•†æ ‡æ³¨å†Œ
    - æŠ€æœ¯è½¬ç§», æˆæœè½¬åŒ–, æŠ€æœ¯äº¤æ˜“

  procurement:
    - æ”¿åºœé‡‡è´­, é¦–è´­, é¦–å°å¥—, ç¤ºèŒƒåº”ç”¨

  standard:
    - æ ‡å‡†åˆ¶å®š, è¡Œä¸šè§„èŒƒ, å‡†å…¥æ¡ä»¶
    - è®¤è¯è®¤å¯, èµ„è´¨ç®¡ç†

policy_targets:
  entities:
    - ä¼ä¸š, æ°‘è¥ä¼ä¸š, å›½æœ‰ä¼ä¸š, å¤–èµ„ä¼ä¸š
    - é«˜æ ¡, ç§‘ç ”é™¢æ‰€, ç ”ç©¶æœºæ„
    - å¹³å°ä¼ä¸š, ç‹¬è§’å…½ä¼ä¸š, ä¸“ç²¾ç‰¹æ–°

  sectors:
    - äººå·¥æ™ºèƒ½, å¤§æ•°æ®, äº‘è®¡ç®—, ç‰©è”ç½‘
    - æ–°èƒ½æº, æ–°ææ–™, ç”Ÿç‰©åŒ»è¯
    - é›†æˆç”µè·¯, é«˜ç«¯è£…å¤‡, èˆªç©ºèˆªå¤©

strength_indicators:
  strong_constraint:
    - å¿…é¡», åº”å½“, ä¸¥æ ¼, å¼ºåˆ¶, é—®è´£, è€ƒæ ¸
    - ä¸å¾—, ç¦æ­¢, é™åˆ¶

  moderate:
    - æ˜ç¡®, è§„å®š, è¦æ±‚, è½å®

  advisory:
    - é¼“åŠ±, æ”¯æŒ, å¼•å¯¼, å€¡å¯¼, æ¨åŠ¨
```

### é™„å½•B: è¯„æµ‹æ•°æ®é›†è¯´æ˜

```yaml
evaluation_datasets:
  development_set:
    size: 500
    purpose: "è°ƒå‚ã€æ¨¡æ¿ä¼˜åŒ–ã€æ ¡å‡†"

  test_set:
    size: 500
    purpose: "æœ€ç»ˆè¯„æµ‹ã€æ¶ˆèå®éªŒ"
    constraint: "ä¸å¾—ç”¨äºè®­ç»ƒæˆ–è°ƒå‚"

  ares_eval_set:
    size: 200
    purpose: "RAGè´¨é‡è‡ªåŠ¨åŒ–è¯„æµ‹"
    annotation: "äººå·¥æ ‡æ³¨æ£€ç´¢ç›¸å…³æ€§ä¸ç­”æ¡ˆå¿ å®åº¦"
```

### é™„å½•C: å‚è€ƒæ–‡çŒ®

```yaml
key_references:
  dapt_tapt:
    - "Gururangan et al., 2020 (ACL): Don't Stop Pretraining"
    - "URL: https://aclanthology.org/2020.acl-main.740/"

  rag:
    - "Lewis et al., 2020 (NeurIPS): Retrieval-Augmented Generation"
    - "URL: https://arxiv.org/abs/2005.11401"

  calibration:
    - "Guo et al., 2017 (ICML): On Calibration of Modern Neural Networks"
    - "URL: https://arxiv.org/abs/1706.04599"

  conformal:
    - "Angelopoulos & Bates: A Gentle Introduction to Conformal Prediction"
    - "URL: https://people.eecs.berkeley.edu/~angelopoulos/publications/downloads/gentle_intro_conformal_dfuq.pdf"

  ares:
    - "Saad-Falcon et al., 2023: ARES Automated Evaluation Framework"
    - "URL: https://arxiv.org/abs/2311.09476"

  owasp:
    - "OWASP LLM Top 10"
    - "URL: https://owasp.org/www-project-top-10-for-large-language-model-applications/"
```

---

## æ€»ç»“

æœ¬è¯­ä¹‰æŠ½å–æ–¹æ¡ˆæä¾›äº†**å®Œæ•´ã€å¯æ‰§è¡Œã€å¯éªŒè¯**çš„å®æ–½è·¯å¾„,ç¡®ä¿:

âœ… **æŠ€æœ¯å®Œæ•´æ€§**: DAPT/TAPT+RAG+Few-shotå…¨æµç¨‹è¦†ç›–
âœ… **è´¨é‡ä¿è¯**: æ¸©åº¦ç¼©æ”¾+å…±å½¢é¢„æµ‹+å¤šå±‚éªŒè¯
âœ… **å¯æ‰§è¡Œæ€§**: Makefile+è„šæœ¬+é…ç½®,åˆ†æ­¥/ä¸€é”®å¯é€‰
âœ… **å¯éªŒè¯æ€§**: æ¶ˆèå®éªŒ+ARESè¯„æµ‹+äººå·¥æŠ½æŸ¥
âœ… **å®‰å…¨åˆè§„**: OWASP LLM Top 10é˜²æŠ¤æªæ–½
âœ… **å¯è¿½æº¯æ€§**: è¯æ®æº¯æº+å®¡è®¡æ—¥å¿—+ç‰ˆæœ¬æ§åˆ¶

**ä¸‹ä¸€æ­¥è¡ŒåŠ¨**:
1. âœ… Day 1-2: æ‰§è¡Œ`make setup`åˆå§‹åŒ–ç¯å¢ƒ
2. âœ… Day 3-5: å‡†å¤‡DAPTè¯­æ–™ä¸é‡‘æ ‡æ ‡æ³¨
3. âœ… Day 6-8: è¿è¡Œ`make dapt`è®­ç»ƒåŸŸé€‚é…æ¨¡å‹
4. âœ… Day 9-10: è¿è¡Œ`make tapt`è®­ç»ƒä»»åŠ¡é€‚é…æ¨¡å‹
5. âœ… Day 11: æ„å»ºRAGç´¢å¼•`make index`
6. âœ… Day 12-13: æ‰¹é‡æŠ½å–`make extract`
7. âœ… Day 14: è´¨é‡éªŒè¯ä¸æ¶ˆèå®éªŒ

**è”ç³»æ–¹å¼**:
- æŠ€æœ¯æ”¯æŒ: nlp@psc-graph.org
- é¡¹ç›®ç®¡ç†: pm@psc-graph.org

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**ç”Ÿæˆæ—¶é—´**: 2025-11-11
**ç»´æŠ¤è€…**: PSC-Graph NLPå·¥ç¨‹ç»„
**æœ€åæ›´æ–°**: 2025-11-11
