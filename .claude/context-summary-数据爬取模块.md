## 项目上下文摘要(数据爬取模块实施)
生成时间:2025-11-17

### 1. 项目背景分析
**项目名称**: PSC-Graph政策语义因果图谱
**当前模块**: 01_数据爬取方案
**项目现状**:
- 已有详细的数据爬取方案文档(01_数据爬取方案.md)
- 项目为全新启动,暂无代码实现
- 需要严格遵循CLAUDE.md中的强制规范

### 2. 核心约束与规范

#### 2.1 语言强制规范
- **绝对强制**: 所有注释、日志、文档必须使用简体中文
- **唯一例外**: 代码标识符(变量名、函数名、类名)使用英文

#### 2.2 合规性强制要求
```yaml
数据合规:
  - robots.txt: 必须遵守
  - QPS限制:
    - gov.cn: ≤1.0 req/s
    - 省级网站: ≤0.7 req/s
    - 统计局API: ≤0.3 req/s
  - 数据来源: 仅公开政府数据
  - 用户代理: "PSC-Graph/0.1 (+research; contact: policy@psc-graph.org)"
```

#### 2.3 工程化强制要求
```yaml
必须实现:
  - 断点续爬: JSON checkpoint持久化
  - SHA256去重: 内容哈希防重复
  - 日志规范: UTF-8编码,简体中文
  - 异常重试: 指数退避,最大重试次数
  - 节流控制: QPS限速+随机抖动
```

#### 2.4 验证强制要求
- **本地验证**: 禁止CI/远程流水线
- **人工抽查**:
  - 政策文本: 10条随机样本,≥95%一致性
  - 统计数据: 27个数据点,误差≤0.1%
  - CNIPA数据: 2个月份,合计值误差≤0.1%

### 3. 数据源清单

#### 3.1 中央政策层
```yaml
来源: 国务院政策文件库
URL: https://www.gov.cn/zhengce/zhengcewenjianku/
栏目:
  - 部门文件: home_{page}.htm
  - 政策解读: jiedu/home_{page}.htm
时间范围: 2009-至今
目标量: ≥500条(首轮)
优先级: 🔴 最高
```

#### 3.2 省级政策层
```yaml
优先批次1(示范):
  - 广东省科技厅: ≥200条(全量)
  - 北京/上海/浙江/江苏: ≥50条/省(示范)
扩展批次2:
  - 31省市全覆盖
优先级: 🔴 最高
```

#### 3.3 产业指标层
```yaml
来源: 国家统计局
核心指标:
  - GDP(现价/不变价/季度)
  - R&D经费支出与强度
  - 规上工业增加值
  - 高技术制造业增加值
  - 研发人员FTE
粒度: 省级-年/季/月
目标量: ≥2000行
优先级: 🟡 高
```

#### 3.4 专利统计层
```yaml
来源: CNIPA月报/年报
数据类型:
  - 发明/实用新型/外观设计授权量
  - PCT申请受理量
分省: 31省月度/年度
目标量: ≥1000行
优先级: 🟡 高
```

### 4. 技术选型(强制依赖)

#### 4.1 HTTP与HTML解析
```python
必须使用:
  - requests: HTTP客户端
  - BeautifulSoup4: HTML解析
  - urllib.robotparser: robots.txt解析

禁止使用:
  - Scrapy: 过度工程化
  - Selenium: 资源消耗大
```

#### 4.2 PDF处理
```python
必须使用:
  - pdfplumber: PDF表格抽取(优先)

备选方案:
  - camelot-py: 复杂表格(需Java)
  - pytesseract: OCR处理扫描件
```

#### 4.3 数据处理
```python
必须使用:
  - pandas: 数据处理
  - hashlib: SHA256哈希
  - PyYAML: 配置文件
```

### 5. 目录结构标准

```
psc-graph-template/
├── Makefile                           # 构建目标
├── scripts/
│   ├── bootstrap.sh                   # 环境初始化
│   ├── requirements.txt               # Python依赖
│   ├── crawler_common.py              # 通用爬虫逻辑
│   ├── crawl_gov_central.py           # 中央政策
│   ├── crawl_provinces.py             # 省级政策
│   ├── fetch_nbs_panel.py             # 统计数据
│   ├── fetch_cnipa_reports.py         # CNIPA月报/年报
│   └── parse_cnipa_pdf_tables.py      # PDF解析
├── data/
│   ├── seeds/seeds_sites.yaml         # 爬虫种子配置
│   ├── province_codes.csv             # 省份编码映射
│   ├── nbs_raw/                       # 统计局原始JSON
│   ├── cnipa_raw/                     # CNIPA原始PDF
│   ├── nbs_panel_long.csv             # 统计指标长表
│   └── cnipa_panel_long.csv           # 专利指标长表
├── corpus/
│   └── raw/
│       ├── policy_central/            # 中央政策
│       └── policy_prov/               # 省级政策
└── results/
    ├── logs/                          # 日志文件
    └── checkpoints/                   # 断点续爬状态
```

### 6. 质量验收标准

#### 6.1 数据量标准
```yaml
最小可运行数据:
  - 中央政策: ≥500条
  - 省级政策: ≥200条(广东)+ ≥150条(其他示范省)
  - 统计数据: ≥2000行
  - 专利数据: ≥1000行
总计: ≥4800条记录
```

#### 6.2 质量标准
```yaml
必须达到:
  - 字段完整性: 缺失率≤1%
  - SHA256去重: 去重率≥99%
  - 人工抽查-政策: ≥95%一致
  - 人工抽查-统计: 误差≤0.1%
  - 人工抽查-CNIPA: 误差≤0.1%
  - source_url有效性: ≥99%
  - Robots遵守: 违规次数=0
  - QPS超限: 超限次数=0
```

### 7. 实施风险点

#### 7.1 合规风险
- **风险**: 过度爬取触发封禁
- **规避**: QPS限速+随机抖动+指数退避

#### 7.2 数据质量风险
- **风险**: 统计局指标编码变更
- **规避**: 浏览器F12验证+人工核对

#### 7.3 PDF解析风险
- **风险**: 扫描件无法识别
- **规避**: pdfplumber优先,OCR备选

#### 7.4 断点续爬风险
- **风险**: checkpoint文件损坏
- **规避**: JSON持久化+幂等性设计

### 8. 关键决策点

#### 8.1 实施优先级顺序
**推荐**: 环境准备 → 中央政策 → 省级政策 → 统计数据 → CNIPA数据
**理由**:
1. 中央政策量大且结构统一,先攻坚建立模式
2. 省级政策复用中央逻辑,仅需适配分页
3. 统计/CNIPA为结构化数据,相对简单

#### 8.2 可用性验证方式
**优先**: 浏览器手动访问+F12抓包
**必须验证**:
- robots.txt是否允许
- 分页链接是否有效
- 统计局指标编码是否正确
- CNIPA PDF是否可下载

#### 8.3 代码复用策略
**核心原则**: 通用逻辑抽象到crawler_common.py
**可复用组件**:
- polite_get(): 节流HTTP请求
- check_robots(): robots.txt检查
- save_checkpoint(): 断点保存
- load_checkpoint(): 断点恢复
- calculate_sha256(): 内容哈希
- retry_with_backoff(): 指数退避重试

### 9. 时间表估算

```yaml
Week 1 (5工作日):
  Day 1-2: 环境准备+seeds配置+通用模块
  Day 3-5: 中央政策爬取+人工验证

Week 2 (5工作日):
  Day 6-7: 省级政策(广东+示范省)
  Day 8-9: 统计数据+CNIPA数据
  Day 10: 质量验收+文档交付
```

### 10. 依赖与前置条件

#### 10.1 系统依赖
- Python 3.9+
- Java 17+ (仅camelot需要,可选)

#### 10.2 网络要求
- 稳定的互联网连接
- 无代理或允许政府网站访问

#### 10.3 人力要求
- 数据工程师×2
- 质量验证工程师×1

### 11. 成功标准

**必须全部满足**:
- ✅ 所有质量验收指标通过
- ✅ 合规性检查无违规记录
- ✅ 代码遵循CLAUDE.md所有强制规范
- ✅ 文档完整(代码+配置+日志+验收报告)
- ✅ 本地验证通过
- ✅ 人工抽查通过

### 12. 关键参考文档

- `/home/user/guanli/CLAUDE.md`: 项目开发强制准则
- `/home/user/guanli/01_数据爬取方案.md`: 数据爬取详细方案
- 无既有代码可参考(全新项目)
