# 04_å›¾å­¦ä¹ æ–¹æ¡ˆ

## æ–‡æ¡£ä¿¡æ¯

- **é¡¹ç›®åç§°**: æ”¿ç­–è¯­ä¹‰å› æœå›¾è°±(PSC-Graph)
- **æ¨¡å—**: å›¾å­¦ä¹ å±‚(å¼‚è´¨æ—¶åºå›¾è°±æ„å»º)
- **ç‰ˆæœ¬**: v1.0
- **æ›´æ–°æ—¥æœŸ**: 2025-11-11
- **è´Ÿè´£äºº**: å›¾å­¦ä¹ å·¥ç¨‹ç»„
- **å‰ç½®ä¾èµ–**: [02_è¯­ä¹‰æŠ½å–æ–¹æ¡ˆ.md](02_è¯­ä¹‰æŠ½å–æ–¹æ¡ˆ.md) å®Œæˆ

---

## ä¸€ã€æ¦‚è§ˆä¸ç›®æ ‡

### 1.1 ä¸šåŠ¡ç›®æ ‡

æœ¬æ–¹æ¡ˆæ˜¯PSC-Graphé¡¹ç›®çš„å›¾è¡¨å¾å­¦ä¹ æ¨¡å—,æ—¨åœ¨å°†ç»“æ„åŒ–çš„æ”¿ç­–è¦ç´ é€šè¿‡å¼‚è´¨æ—¶åºå›¾å­¦ä¹ ,å»ºç«‹"æ”¿ç­–è¯­ä¹‰â†’äº§ä¸šè¡Œä¸º"çš„å¯è§£é‡Šè·¯å¾„ã€‚

**æ ¸å¿ƒä»»åŠ¡**:
- âœ… **å¼‚è´¨å›¾æ„å»º**: å»ºç«‹æ”¿ç­–-ä¼ä¸š-åœ°åŒº-æŠ€æœ¯-èµ„é‡‘å¤šç±»å‹èŠ‚ç‚¹å›¾
- âœ… **æ—¶åºç¼–ç **: é€šè¿‡Bochneræ—¶é—´ç¼–ç æ•æ‰æ”¿ç­–æ¼”åŒ–
- âœ… **HGTå­¦ä¹ **: å¼‚è´¨å›¾Transformerå­¦ä¹ èŠ‚ç‚¹è¡¨å¾
- âœ… **TGATå¢å¼º**: æ—¶åºå›¾æ³¨æ„åŠ›ç½‘ç»œå»ºæ¨¡åŠ¨æ€å…³ç³»
- âœ… **è·¯å¾„å¯è§£é‡Š**: é«˜æƒé‡è·¯å¾„å›æŸ¥åˆ°RAGè¯æ®

### 1.2 æŠ€æœ¯è·¯çº¿

```mermaid
graph LR
    A[ç»“æ„åŒ–è¦ç´ ] --> B[å¼‚è´¨å›¾æ„å»º]
    B --> C[ç‰¹å¾å·¥ç¨‹]
    C --> D[HGTå¼‚è´¨å­¦ä¹ ]
    D --> E[TGATæ—¶åºç¼–ç ]
    E --> F[é“¾è·¯é¢„æµ‹/èŠ‚ç‚¹åˆ†ç±»]
    F --> G[æ³¨æ„åŠ›æƒé‡åˆ†æ]
    G --> H[è·¯å¾„å¯è§£é‡Šæ€§]
```

**å…³é”®å‡è®¾(H2)**:
åŸºäº**å¼‚è´¨+æ—¶åºå›¾å­¦ä¹ (HGT+TGAT)**çš„æ”¿ç­–-äº§ä¸šå›¾è°±èƒ½æä¾›å¯è§£é‡Šçš„è¯­ä¹‰-è¡Œä¸ºè·¯å¾„,æ˜¾è‘—ä¼˜äºåŒè´¨å›¾æˆ–éæ—¶åºæ¨¡å‹ã€‚

**å‚è€ƒæ–‡çŒ®**:
- Hu et al., KDD 2020: "Heterogeneous Graph Transformer"
- Xu et al., ICLR 2020: "Inductive Representation Learning on Temporal Graphs"

### 1.3 è¾“å…¥è¾“å‡ºè§„èŒƒ

| é¡¹ç›® | è¯´æ˜ |
|-----|------|
| **è¾“å…¥** | ç»“æ„åŒ–äº”å…ƒç»„(extractions/*.json)ã€äº§ä¸šæŒ‡æ ‡é¢æ¿(data/panel_base.csv) |
| **è¾“å‡º** | èŠ‚ç‚¹åµŒå…¥(embeddings/*.pt)ã€é“¾è·¯é¢„æµ‹ç»“æœã€æ³¨æ„åŠ›æƒé‡ |
| **è´¨é‡é—¨æ§›** | é“¾è·¯é¢„æµ‹AUCâ‰¥0.80ã€èŠ‚ç‚¹åˆ†ç±»Macro-F1â‰¥0.75 |
| **é¢„è®¡æ—¶é—´** | 2å‘¨(W5-W6,10ä¸ªå·¥ä½œæ—¥) |

---

## äºŒã€å¼‚è´¨å›¾æ„å»º

### 2.1 å›¾Schemaè®¾è®¡

#### ğŸ“Š èŠ‚ç‚¹ç±»å‹å®šä¹‰

```yaml
node_types:
  Policy:
    description: "æ”¿ç­–æ–‡æ¡£èŠ‚ç‚¹"
    id_format: "policy_{doc_id}"
    features:
      - text_embedding (384-d, sentence-transformers)
      - pub_year (integer)
      - issuer_id (categorical)
      - strength_avg (float, å¹³å‡å¼ºåº¦)

  Actor:
    description: "äº§ä¸šä¸»ä½“èŠ‚ç‚¹(ä¼ä¸š/é«˜æ ¡/ç§‘ç ”é™¢æ‰€)"
    id_format: "actor_{name_hash}"
    features:
      - entity_type (enum: enterprise|university|institute)
      - location (geo-encoding)

  Region:
    description: "åœ°åŒºèŠ‚ç‚¹(çœ/å¸‚/åŒº)"
    id_format: "region_{admin_code}"
    features:
      - gdp (float)
      - r&d_intensity (float)
      - population (integer)

  Topic:
    description: "æŠ€æœ¯ä¸»é¢˜èŠ‚ç‚¹(ä»goalæå–)"
    id_format: "topic_{keyword_hash}"
    features:
      - keyword_embedding (384-d)

  Funding:
    description: "èµ„é‡‘/å¹³å°èŠ‚ç‚¹"
    id_format: "fund_{name_hash}"
    features:
      - amount (float)
      - type (enum: funding|platform)
```

#### ğŸ”— è¾¹ç±»å‹å®šä¹‰

```yaml
edge_types:
  (Policy, publish, Region):
    description: "æ”¿ç­–å‘å¸ƒå…³ç³»"
    attributes:
      - pub_date (timestamp)
      - weight (1.0)

  (Policy, apply_to, Actor):
    description: "æ”¿ç­–é€‚ç”¨å…³ç³»"
    attributes:
      - strength (0-3)
      - instruments (list)
      - timestamp (effective_date)

  (Policy, target, Topic):
    description: "æ”¿ç­–ç›®æ ‡å…³ç³»"
    attributes:
      - relevance_score (float)

  (Actor, located_in, Region):
    description: "ä¸»ä½“ä½ç½®å…³ç³»"
    attributes:
      - since (year)

  (Funding, benefit, Actor):
    description: "èµ„é‡‘å—ç›Šå…³ç³»"
    attributes:
      - amount (float)
      - timestamp (grant_date)

  (Actor, collaborate, Actor):
    description: "ä¸»ä½“åä½œå…³ç³»"
    attributes:
      - co_occurrence_count (integer)
```

### 2.2 å›¾æ„å»ºè„šæœ¬

```python
# scripts/build_heterograph.py
import json, glob, torch, hashlib
from torch_geometric.data import HeteroData
from sentence_transformers import SentenceTransformer

# åŠ è½½åµŒå…¥æ¨¡å‹
model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')

def hash_id(text):
    """ç”Ÿæˆç¨³å®šçš„hash ID"""
    return hashlib.md5(text.encode('utf-8')).hexdigest()[:16]

def build_graph():
    """ä»æŠ½å–ç»“æœæ„å»ºå¼‚è´¨å›¾"""
    data = HeteroData()

    # èŠ‚ç‚¹å­—å…¸
    policy_map, actor_map, region_map, topic_map, fund_map = {}, {}, {}, {}, {}

    def get_or_create(mp, key, node_type):
        if key not in mp:
            mp[key] = len(mp)
        return mp[key]

    # è¾¹åˆ—è¡¨
    edges = {
        ('policy', 'publish', 'region'): [],
        ('policy', 'apply_to', 'actor'): [],
        ('policy', 'target', 'topic'): [],
        ('actor', 'located_in', 'region'): [],
        ('funding', 'benefit', 'actor'): []
    }

    # éå†æŠ½å–ç»“æœ
    for path in glob.glob("extractions/*.json"):
        doc = json.load(open(path, "r", encoding="utf-8"))

        # PolicyèŠ‚ç‚¹
        policy_id = f"policy_{doc['doc_id']}"
        pidx = get_or_create(policy_map, policy_id, 'policy')

        # RegionèŠ‚ç‚¹
        if 'region_scope' in doc and doc['region_scope']:
            region_id = f"region_{doc['region_scope'].get('admin_code', 'unknown')}"
            ridx = get_or_create(region_map, region_id, 'region')
            edges[('policy', 'publish', 'region')].append([pidx, ridx])

        # éå†æ¡æ¬¾æ ‡æ³¨
        for clause in doc.get('clauses', []):
            for ann in clause.get('annotations', []):
                # ActorèŠ‚ç‚¹
                actor_id = f"actor_{hash_id(ann['target_actor'])}"
                aidx = get_or_create(actor_map, actor_id, 'actor')
                edges[('policy', 'apply_to', 'actor')].append([pidx, aidx])

                # TopicèŠ‚ç‚¹
                topic_id = f"topic_{hash_id(ann['goal'][:50])}"
                tidx = get_or_create(topic_map, topic_id, 'topic')
                edges[('policy', 'target', 'topic')].append([pidx, tidx])

                # FundingèŠ‚ç‚¹
                for sup in ann.get('support', []):
                    if sup['type'] in ['funding', 'platform']:
                        fund_id = f"fund_{hash_id(sup.get('note', 'unnamed'))}"
                        fidx = get_or_create(fund_map, fund_id, 'funding')
                        edges[('funding', 'benefit', 'actor')].append([fidx, aidx])

    # æ„é€ PyG HeteroData
    data['policy'].num_nodes = len(policy_map)
    data['actor'].num_nodes = len(actor_map)
    data['region'].num_nodes = len(region_map)
    data['topic'].num_nodes = len(topic_map)
    data['funding'].num_nodes = len(fund_map)

    # åˆå§‹åŒ–ç‰¹å¾(ç®€åŒ–:éšæœºåˆå§‹åŒ–,å®é™…åº”åŠ è½½çœŸå®ç‰¹å¾)
    for ntype in data.node_types:
        num_nodes = data[ntype].num_nodes
        data[ntype].x = torch.randn(num_nodes, 128)

    # æ·»åŠ è¾¹
    for edge_type, edge_list in edges.items():
        if len(edge_list) > 0:
            ei = torch.tensor(edge_list, dtype=torch.long).t().contiguous()
            data[edge_type].edge_index = ei

    # ä¿å­˜
    torch.save(data, "data/hetero_graph.pt")
    print(f"[Graph] Nodes: {sum(d.num_nodes for _, d in data.node_items())}")
    print(f"[Graph] Edges: {sum(ei.size(1) for _, _, ei in data.edge_items())}")

    return data

if __name__ == "__main__":
    graph = build_graph()
```

---

## ä¸‰ã€HGTå¼‚è´¨å›¾å­¦ä¹ 

### 3.1 æ¨¡å‹æ¶æ„

```python
# scripts/models/hgt_model.py
import torch
import torch.nn.functional as F
from torch_geometric.nn import HGTConv, Linear

class HGTModel(torch.nn.Module):
    def __init__(self, metadata, hidden_channels=128, out_channels=64, num_heads=4, num_layers=2):
        super().__init__()

        self.lin_dict = torch.nn.ModuleDict()
        for node_type in metadata[0]:
            self.lin_dict[node_type] = Linear(-1, hidden_channels)

        self.convs = torch.nn.ModuleList()
        for _ in range(num_layers):
            conv = HGTConv(hidden_channels, hidden_channels, metadata,
                           num_heads, group='sum')
            self.convs.append(conv)

        self.lin_out = Linear(hidden_channels, out_channels)

    def forward(self, x_dict, edge_index_dict):
        # è¾“å…¥æŠ•å½±
        x_dict = {
            key: F.gelu(self.lin_dict[key](x))
            for key, x in x_dict.items()
        }

        # HGTå·ç§¯å±‚
        for conv in self.convs:
            x_dict = conv(x_dict, edge_index_dict)
            x_dict = {key: F.gelu(x) for key, x in x_dict.items()}

        # è¾“å‡ºæŠ•å½±
        x_dict = {key: self.lin_out(x) for key, x in x_dict.items()}

        return x_dict
```

### 3.2 è®­ç»ƒé…ç½®

```yaml
training_config:
  task: "é“¾è·¯é¢„æµ‹ + èŠ‚ç‚¹åˆ†ç±»"

  hyperparameters:
    hidden_channels: 128
    out_channels: 64
    num_heads: 4
    num_layers: 2
    learning_rate: 0.001
    weight_decay: 0.0001
    dropout: 0.2

  data_split:
    train: "æ—¶é—´<2020å¹´çš„è¾¹"
    val: "2020å¹´çš„è¾¹"
    test: "æ—¶é—´>2020å¹´çš„è¾¹"

  hardware:
    gpus: "1x RTX 3090 24GB"
    batch_size: 256
    neighbor_sampling: [10, 10]  # 2-hopé‚»åŸŸé‡‡æ ·

  numerical_stability:  # v1.1.0+ æ–°å¢æ•°å€¼ç¨³å®šæ€§é…ç½®
    max_grad_norm: 1.0      # æ¢¯åº¦è£å‰ªé˜ˆå€¼ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
    logits_clamp_range: [-10.0, 10.0]  # Logitsè£å‰ªèŒƒå›´ï¼Œé˜²æ­¢BCEäº§ç”ŸNaN
    enable_nan_detection: true  # å¯ç”¨NaNæ£€æµ‹æœºåˆ¶
    use_inplace_ops: false  # ç¦ç”¨in-placeæ“ä½œï¼ˆå¦‚relu_()ï¼‰
```

### 3.2.1 æ•°å€¼ç¨³å®šæ€§ä¸æ¢¯åº¦é—®é¢˜

**é—®é¢˜èƒŒæ™¯**ï¼š

æ·±åº¦å­¦ä¹ è®­ç»ƒä¸­å¸¸è§çš„æ•°å€¼é—®é¢˜åŒ…æ‹¬ï¼š
1. **NaNæŸå¤±**ï¼šå½“logitså€¼è¿‡å¤§ï¼ˆ>100ï¼‰æ—¶ï¼Œbinary_cross_entropy_with_logitså¯èƒ½äº§ç”ŸNaN
2. **æ¢¯åº¦çˆ†ç‚¸**ï¼šå­¦ä¹ ç‡è¿‡å¤§æˆ–æ®‹å·®è¿æ¥ç´¯ç§¯å¯¼è‡´æ¢¯åº¦æŒ‡æ•°å¢é•¿
3. **æ¢¯åº¦æ¶ˆå¤±**ï¼šin-placeæ“ä½œï¼ˆå¦‚`relu_()`ï¼‰å¯èƒ½ç ´åæ¢¯åº¦è®¡ç®—é“¾
4. **æ•°å€¼æº¢å‡º**ï¼šæ—¶é—´ç¼–ç ä¸­æå¤§çš„å¤©æ•°å€¼å¯¼è‡´sin/cosè®¡ç®—ä¸ç¨³å®š

**è§£å†³æ–¹æ¡ˆï¼ˆv1.1.0+å·²å®ç°ï¼‰**ï¼š

**1. Logitsè£å‰ªï¼ˆtrain_hgt.py:258-259ï¼‰**

```python
# è£å‰ªåˆ°å®‰å…¨èŒƒå›´ï¼Œé˜²æ­¢BCEäº§ç”ŸNaN
pos_scores = torch.clamp(pos_scores, min=-10.0, max=10.0)
neg_scores = torch.clamp(neg_scores, min=-10.0, max=10.0)
```

**è¯´æ˜**ï¼šèŒƒå›´[-10, 10]è¶³å¤Ÿè¡¨è¾¾[4.5e-5, 0.99995]çš„æ¦‚ç‡èŒƒå›´ï¼Œé¿å…æ•°å€¼æº¢å‡ºã€‚

**2. æ¢¯åº¦è£å‰ªï¼ˆtrain_hgt.py:287ï¼‰**

```python
# é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
```

**è¯´æ˜**ï¼šå°†æ‰€æœ‰å‚æ•°æ¢¯åº¦çš„å…¨å±€èŒƒæ•°é™åˆ¶åœ¨`max_grad_norm`ä»¥å†…ï¼ˆé»˜è®¤1.0ï¼‰ã€‚

**3. NaNæ£€æµ‹ä¸æ—©åœï¼ˆtrain_hgt.py:274-295ï¼‰**

```python
# æŸå¤±NaNæ£€æµ‹
if torch.isnan(loss):
    raise RuntimeError(f"æ£€æµ‹åˆ°NaNæŸå¤±ï¼...")

# æ¢¯åº¦NaNæ£€æµ‹
for name, param in model.named_parameters():
    if param.grad is not None and torch.isnan(param.grad).any():
        raise RuntimeError(f"æ£€æµ‹åˆ°NaNæ¢¯åº¦ï¼å‚æ•°: {name}...")
```

**è¯´æ˜**ï¼šæ¯ä¸ªepochè‡ªåŠ¨æ£€æµ‹å¹¶æŠ¥å‘ŠNaNä½ç½®ï¼Œæä¾›è¯¦ç»†è¯Šæ–­ä¿¡æ¯ã€‚

**4. ç¦ç”¨In-placeæ“ä½œï¼ˆtrain_hgt.py:146ï¼‰**

```python
# âŒ é”™è¯¯ï¼šä½¿ç”¨in-place ReLU
h_dict[node_type] = self.lin_dict[node_type](x).relu_()

# âœ… æ­£ç¡®ï¼šä½¿ç”¨éin-place ReLU
h_dict[node_type] = self.lin_dict[node_type](x).relu()
```

**è¯´æ˜**ï¼šin-placeæ“ä½œä¼šä¿®æ”¹è¾“å…¥å¼ é‡ï¼Œå¯èƒ½å¯¼è‡´åå‘ä¼ æ’­æ—¶æ¢¯åº¦è®¡ç®—é”™è¯¯ã€‚

**5. è®¾å¤‡ä¼˜åŒ–ï¼ˆtrain_hgt.py:249-250ï¼‰**

```python
# ç›´æ¥åœ¨ç›®æ ‡è®¾å¤‡ä¸Šç”Ÿæˆéšæœºç´¢å¼•ï¼Œé¿å…CPU-GPUä¼ è¾“
device = edge_index.device
neg_src = torch.randint(0, data[src_type].x.shape[0], (num_neg,), device=device)
```

**6. æ—¶é—´ç¼–ç æ•°å€¼ä¿æŠ¤ï¼ˆbuild_graph_pyg.py:359-366ï¼‰**

```python
# é™åˆ¶daysåœ¨åˆç†èŒƒå›´å†…ï¼Œé˜²æ­¢æ•°å€¼æº¢å‡º
# èŒƒå›´ï¼š[-3650, 3650]å¯¹åº”[2010-01-01, 2030-01-01]
if abs(days) > 3650:
    days = max(-3650, min(3650, days))
```

**è¯Šæ–­ä¸è°ƒä¼˜æŒ‡å—**ï¼š

| é—®é¢˜ç°è±¡ | å¯èƒ½åŸå›  | å»ºè®®è°ƒæ•´ |
|---------|---------|---------|
| Lossçªç„¶å˜NaN | Logitsè¿‡å¤§ | é™ä½å­¦ä¹ ç‡åˆ°0.0001 |
| æ¢¯åº¦èŒƒæ•°>100 | æ¢¯åº¦çˆ†ç‚¸ | å‡å°max_grad_normåˆ°0.5 |
| Lossä¸ä¸‹é™ | å­¦ä¹ ç‡è¿‡å° | å¢åŠ åˆ°0.001æˆ–0.01 |
| ç‰¹å®šå‚æ•°æ¢¯åº¦NaN | è¾“å…¥å¼‚å¸¸ | æ£€æŸ¥èŠ‚ç‚¹ç‰¹å¾æ˜¯å¦åŒ…å«NaN/Inf |
| æ—¶é—´æˆ³è§£æå¤±è´¥>10% | æ•°æ®è´¨é‡é—®é¢˜ | æ¸…æ´—dataæºæˆ–ä¿®æ­£æ—¶é—´æ ¼å¼ |

**ç›‘æ§å»ºè®®**ï¼ˆæ·»åŠ åˆ°è®­ç»ƒå¾ªç¯ï¼‰ï¼š

```python
if epoch % 5 == 0:
    # å‚æ•°èŒƒæ•°
    param_norm = sum(p.norm().item() for p in model.parameters())
    # æ¢¯åº¦èŒƒæ•°
    grad_norm = sum(p.grad.norm().item() for p in model.parameters()
                   if p.grad is not None)
    print(f"Epoch {epoch} | Loss: {loss:.4f} | Param: {param_norm:.2f} | Grad: {grad_norm:.2f}")
```

### 3.3 è®­ç»ƒè„šæœ¬

```python
# scripts/train_hgt.py
import torch
from torch_geometric.loader import NeighborLoader
from models.hgt_model import HGTModel

def train_epoch(model, loader, optimizer):
    model.train()
    total_loss = 0

    for batch in loader:
        optimizer.zero_grad()

        # å‰å‘ä¼ æ’­
        out = model(batch.x_dict, batch.edge_index_dict)

        # é“¾è·¯é¢„æµ‹æŸå¤±(ç®€åŒ–ç¤ºä¾‹)
        # å®é™…åº”ä½¿ç”¨è´Ÿé‡‡æ ·æˆ–å¯¹æ¯”å­¦ä¹ 
        loss = F.mse_loss(out['policy'][:10], torch.randn(10, 64))

        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    return total_loss / len(loader)

def main():
    # åŠ è½½å›¾æ•°æ®
    data = torch.load("data/hetero_graph.pt")

    # åˆå§‹åŒ–æ¨¡å‹
    model = HGTModel(data.metadata(), hidden_channels=128)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    # è®­ç»ƒå¾ªç¯
    for epoch in range(100):
        loss = train_epoch(model, [data], optimizer)  # ç®€åŒ–:å…¨å›¾è®­ç»ƒ
        print(f"Epoch {epoch:03d}, Loss: {loss:.4f}")

    # ä¿å­˜æ¨¡å‹
    torch.save(model.state_dict(), "models/hgt_checkpoint.pt")

if __name__ == "__main__":
    main()
```

---

## å››ã€TGATæ—¶åºå¢å¼º

### 4.1 æ—¶é—´ç¼–ç 

```python
# scripts/models/tgat_model.py
import torch
import math

class BochnerTimeEncoder(torch.nn.Module):
    def __init__(self, time_dim=32):
        super().__init__()
        self.time_dim = time_dim
        self.w = torch.nn.Linear(1, time_dim)

    def forward(self, timestamps):
        """
        timestamps: (num_edges,) Unixæ—¶é—´æˆ³
        è¿”å›: (num_edges, time_dim) æ—¶é—´ç¼–ç 
        """
        t = timestamps.unsqueeze(-1).float()  # (num_edges, 1)
        omega = self.w(t)  # (num_edges, time_dim)

        # Bochnerå®šç†: e^{i*omega*t} çš„å®éƒ¨å’Œè™šéƒ¨
        time_enc = torch.cat([torch.cos(omega), torch.sin(omega)], dim=-1)
        return time_enc  # (num_edges, 2*time_dim)
```

### 4.2 TGATé›†æˆ

```python
class HGT_TGAT_Model(torch.nn.Module):
    def __init__(self, metadata, time_dim=32, hidden_channels=128):
        super().__init__()
        self.time_encoder = BochnerTimeEncoder(time_dim)
        self.hgt = HGTModel(metadata, hidden_channels)

    def forward(self, x_dict, edge_index_dict, edge_time_dict):
        # æ—¶é—´ç¼–ç 
        time_features = {}
        for edge_type, timestamps in edge_time_dict.items():
            time_features[edge_type] = self.time_encoder(timestamps)

        # å°†æ—¶é—´ç‰¹å¾èåˆåˆ°èŠ‚ç‚¹ç‰¹å¾(ç®€åŒ–)
        # å®é™…åº”åœ¨HGTçš„æ¶ˆæ¯ä¼ é€’ä¸­èåˆ

        # HGTå‰å‘ä¼ æ’­
        out = self.hgt(x_dict, edge_index_dict)

        return out
```

---

## äº”ã€è¯„æµ‹ä¸æ¶ˆè

### 5.1 é“¾è·¯é¢„æµ‹

```python
# scripts/evaluate_link_prediction.py
from sklearn.metrics import roc_auc_score, average_precision_score

def evaluate_link_prediction(model, test_edges, negative_edges):
    """è¯„ä¼°é“¾è·¯é¢„æµ‹æ€§èƒ½"""
    model.eval()

    with torch.no_grad():
        # æ­£æ ·æœ¬å¾—åˆ†
        pos_scores = []
        for src, dst in test_edges:
            score = (model.embeddings[src] * model.embeddings[dst]).sum()
            pos_scores.append(score.item())

        # è´Ÿæ ·æœ¬å¾—åˆ†
        neg_scores = []
        for src, dst in negative_edges:
            score = (model.embeddings[src] * model.embeddings[dst]).sum()
            neg_scores.append(score.item())

    # è®¡ç®—AUCå’ŒAP
    y_true = [1] * len(pos_scores) + [0] * len(neg_scores)
    y_score = pos_scores + neg_scores

    auc = roc_auc_score(y_true, y_score)
    ap = average_precision_score(y_true, y_score)

    print(f"[Link Prediction] AUC={auc:.3f}, AP={ap:.3f}")
    return auc, ap
```

### 5.2 æ¶ˆèå®éªŒ

```yaml
ablation_experiments:
  baseline_gcn:
    description: "åŒè´¨GCN(å°†æ‰€æœ‰èŠ‚ç‚¹è§†ä¸ºåŒç±»)"
    expected: "æ€§èƒ½ä½äºHGT"

  hgt_only:
    description: "HGTæ— æ—¶åºç¼–ç "
    expected: "æ€§èƒ½ä½äºHGT+TGAT"

  tgat_only:
    description: "TGATæ— å¼‚è´¨ç±»å‹"
    expected: "æ€§èƒ½ä½äºHGT+TGAT"

  no_rag_evidence:
    description: "èŠ‚ç‚¹ç‰¹å¾éšæœºåˆå§‹åŒ–(ä¸ç”¨RAGæ–‡æœ¬åµŒå…¥)"
    expected: "æ€§èƒ½æ˜¾è‘—ä¸‹é™"

  full_model:
    description: "HGT+TGAT+RAGè¯æ®"
    expected: "æœ€ä½³æ€§èƒ½"
```

---

## å…­ã€è·¯å¾„å¯è§£é‡Šæ€§

### 6.1 æ³¨æ„åŠ›æƒé‡åˆ†æ

```python
# scripts/analyze_attention.py
def extract_attention_weights(model, edge_index_dict):
    """æå–HGTæ³¨æ„åŠ›æƒé‡"""
    model.eval()

    attention_weights = {}
    for edge_type in edge_index_dict.keys():
        # ä»HGTå·ç§¯å±‚æå–æ³¨æ„åŠ›
        # å®é™…éœ€è¦ä¿®æ”¹HGTConvä»¥è¿”å›attention
        weights = model.convs[0].get_attention(edge_type)
        attention_weights[edge_type] = weights

    return attention_weights

def find_high_weight_paths(attention_weights, k=10):
    """æ‰¾åˆ°é«˜æƒé‡è·¯å¾„"""
    paths = []

    for edge_type, weights in attention_weights.items():
        # æ’åºæ‰¾top-k
        top_k_indices = weights.argsort(descending=True)[:k]

        for idx in top_k_indices:
            src, dst = edge_index_dict[edge_type][:, idx]
            paths.append({
                "edge_type": edge_type,
                "src": src.item(),
                "dst": dst.item(),
                "weight": weights[idx].item()
            })

    return sorted(paths, key=lambda x: x['weight'], reverse=True)[:k]
```

### 6.2 è¯æ®å›æŸ¥

```python
def trace_to_evidence(path, extraction_dir):
    """å°†é«˜æƒé‡è·¯å¾„å›æº¯åˆ°RAGè¯æ®"""
    # ä¾‹å¦‚: policy_123 â†’ actor_456
    policy_id = path['src']
    actor_id = path['dst']

    # åŠ è½½åŸå§‹æŠ½å–ç»“æœ
    doc_path = f"{extraction_dir}/policy_{policy_id}.json"
    doc = json.load(open(doc_path, "r"))

    # æŸ¥æ‰¾å¯¹åº”çš„æ ‡æ³¨å’Œè¯æ®
    for clause in doc['clauses']:
        for ann in clause['annotations']:
            if hash_id(ann['target_actor']) == actor_id:
                # è¿”å›è¯æ®æ–‡æœ¬
                evidence = clause['text'][
                    ann['evidence_spans'][0]['start']:
                    ann['evidence_spans'][0]['end']
                ]
                return evidence

    return None
```

---

## ä¸ƒã€Makefileç›®æ ‡

```makefile
graph:
	@echo "[Graph] Building heterogeneous graph..."
	@$(ACT) python scripts/build_heterograph.py

train_hgt:
	@echo "[HGT] Training HGT model..."
	@$(ACT) python scripts/train_hgt.py

evaluate_graph:
	@echo "[Eval] Evaluating graph learning..."
	@$(ACT) python scripts/evaluate_link_prediction.py

ablation_graph:
	@echo "[Ablation] Running ablation experiments..."
	@$(ACT) bash scripts/run_graph_ablation.sh
```

---

## å…«ã€éªŒæ”¶æ ‡å‡†

```yaml
acceptance_criteria:
  graph_construction:
    nodes: "â‰¥10000ä¸ªèŠ‚ç‚¹"
    edges: "â‰¥50000æ¡è¾¹"
    node_types: "=5ç§(Policy/Actor/Region/Topic/Funding)"

  model_performance:
    link_prediction_auc: "â‰¥0.80"
    node_classification_f1: "â‰¥0.75"

  interpretability:
    high_weight_paths: "â‰¥10æ¡å¯å›æŸ¥åˆ°RAGè¯æ®"
    attention_visualization: "ç”Ÿæˆæ³¨æ„åŠ›çƒ­åŠ›å›¾"

  ablation_validation:
    hgt_vs_gcn: "HGTæ˜¾è‘—ä¼˜äºGCN(p<0.05)"
    with_vs_without_time: "TGATæå‡â‰¥5%"
```

---

## æ€»ç»“

æœ¬å›¾å­¦ä¹ æ–¹æ¡ˆæä¾›äº†**å®Œæ•´ã€å¯æ‰§è¡Œã€å¯è§£é‡Š**çš„å¼‚è´¨æ—¶åºå›¾å­¦ä¹ æµç¨‹,ç¡®ä¿:

âœ… **å¼‚è´¨å»ºæ¨¡**: HGTå¤„ç†å¤šç±»å‹èŠ‚ç‚¹/è¾¹
âœ… **æ—¶åºç¼–ç **: TGAT+Bochneræ•æ‰åŠ¨æ€æ¼”åŒ–
âœ… **å¯è§£é‡Šæ€§**: æ³¨æ„åŠ›æƒé‡â†’RAGè¯æ®
âœ… **ä¸¥æ ¼è¯„æµ‹**: é“¾è·¯é¢„æµ‹+æ¶ˆèå®éªŒ

**è”ç³»æ–¹å¼**: graph@psc-graph.org

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**ç”Ÿæˆæ—¶é—´**: 2025-11-11
**ç»´æŠ¤è€…**: PSC-Graphå›¾å­¦ä¹ ç»„
